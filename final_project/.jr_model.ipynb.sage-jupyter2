{"backend_state":"init","connection_file":"/projects/d5a3aa23-997d-4ffc-977a-1dc20c583e62/.local/share/jupyter/runtime/kernel-43456b18-cecb-4b31-a52d-18ada75ac371.json","kernel":"ds_env","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"7f14ff","input":"from sklearn.model_selection import GridSearchCV\n\nparams = ['kernels': ['poly', 'linear', 'rbf', 'sigmoid'], 'degree': [0, 1, 2, 3, 4, 5, 6], 'c_value': [.1, 1, 10, 100, 1000], 'gamma': [.1, 1, 10, 100]]\nmy_SVC_model.GridSearchCV","pos":30,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"b8439d","input":"","pos":6,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"eb41fc","input":"","pos":19,"type":"cell"}
{"cell_type":"code","exec_count":126,"id":"e7eb4b","input":"mlp = MLPClassifier(solver='lbfgs',hidden_layer_sizes=(1000, 35), alpha=0.05, max_iter=200, random_state = 1, activation='relu', learning_rate='adaptive')\nmlp.fit(x_train, y_train)\ny_hat = mlp.predict(x_test)\nprint(y_hat)\nprint(np.array(y_test))\ntotal_squared_error = (np.sum((y_test - y_hat)**2)) #get the sum of all the errors (error = what we want (y_test) - what we predicted (y_hat))\nmean_squared_error = total_squared_error/len(y_test) #divide this by how many rows/observations we have \nprint(mean_squared_error)\nsns.heatmap(confusion_matrix(y_test, y_hat), annot=True, fmt='g')\nacc = accuracy_score(y_test, y_hat)\nprec = precision_score(y_test, y_hat, average='micro')\nrecall = recall_score(y_test, y_hat, average='micro')\nf1 = f1_score(y_test, y_hat, average='micro')\nscores['mlp'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}\nprint(scores['mlp'])","output":{"0":{"name":"stdout","output_type":"stream","text":"[1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1\n 0 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1\n 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1\n 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 1 0\n 0 0 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1]\n[1 1 1 0 0 1 1 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 0\n 0 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 0 1 0 1 1 1 0 1 0 0 1\n 1 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1\n 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 1 1 1 1 0 1 0 0 0 0 1 1 0\n 0 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1]\n0.1358695652173913\n{'accuracy': 0.8641304347826086, 'precision': 0.8641304347826086, 'recall': 0.8641304347826086, 'f1_score': 0.8641304347826086}\n"},"1":{"name":"stderr","output_type":"stream","text":"/projects/d5a3aa23-997d-4ffc-977a-1dc20c583e62/miniconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"},"2":{"data":{"image/png":"5226a94ba91784c31e10f7452025cade180299d9","text/plain":"<Figure size 432x288 with 2 Axes>"},"exec_count":126,"metadata":{"needs_background":"light"},"output_type":"execute_result"}},"pos":15,"type":"cell"}
{"cell_type":"code","exec_count":144,"id":"9f2cce","input":"# define x as all columns but heart disease\n\nX = heart_df.loc[:, heart_df.columns != \"HeartDisease\"]\nX = (X - np.min(X)) / (np.max(X) - np.min(X))\n#define y as heart_disease\ny = heart_df['HeartDisease']\n#split data set 80 percent train: 20 percent test\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y)\n\n#create object model\ngnb = GaussianNB()\n#fit object model\ngnb.fit(X_train, y_train)\ny_pred = gnb.predict(X_test)\n# print(\"Naive Bayes score: \",gnb.score(X_test, y_test))\n# print(\"Number of mislabeled points out of a total %d points : %d\"\n\n\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\nscores['NAIVEBAYES'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}\nprint(scores['NAIVEBAYES'])\n","output":{"0":{"name":"stderr","output_type":"stream","text":"/projects/d5a3aa23-997d-4ffc-977a-1dc20c583e62/miniconda3/envs/ds_env/lib/python3.8/site-packages/numpy/core/fromnumeric.py:84: FutureWarning: In a future version, DataFrame.min(axis=None) will return a scalar min over the entire DataFrame. To retain the old behavior, use 'frame.min(axis=0)' or just 'frame.min()'\n  return reduction(axis=axis, out=out, **passkwargs)\n/projects/d5a3aa23-997d-4ffc-977a-1dc20c583e62/miniconda3/envs/ds_env/lib/python3.8/site-packages/numpy/core/fromnumeric.py:84: FutureWarning: In a future version, DataFrame.max(axis=None) will return a scalar max over the entire DataFrame. To retain the old behavior, use 'frame.max(axis=0)' or just 'frame.max()'\n  return reduction(axis=axis, out=out, **passkwargs)\n"},"1":{"name":"stdout","output_type":"stream","text":"{'accuracy': 0.8858695652173914, 'precision': 0.8785046728971962, 'recall': 0.9215686274509803, 'f1_score': 0.8995215311004785}\n"},"2":{"data":{"image/png":"deb0b7fb4c9c360871651d14c389926a4b8624e5","text/plain":"<Figure size 432x288 with 2 Axes>"},"exec_count":144,"metadata":{"needs_background":"light"},"output_type":"execute_result"}},"pos":20,"type":"cell"}
{"cell_type":"code","exec_count":152,"id":"605396","input":"#y_hat = KNN_model.predict(x_test)","pos":13,"type":"cell"}
{"cell_type":"code","exec_count":20,"id":"2a6364","input":"heart_df = pd.read_csv(\"data/heart.csv\")\nheart_df\n","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>ChestPainType</th>\n      <th>RestingBP</th>\n      <th>Cholesterol</th>\n      <th>FastingBS</th>\n      <th>RestingECG</th>\n      <th>MaxHR</th>\n      <th>ExerciseAngina</th>\n      <th>Oldpeak</th>\n      <th>ST_Slope</th>\n      <th>HeartDisease</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>40</td>\n      <td>M</td>\n      <td>ATA</td>\n      <td>140</td>\n      <td>289</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>172</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Up</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>49</td>\n      <td>F</td>\n      <td>NAP</td>\n      <td>160</td>\n      <td>180</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>156</td>\n      <td>N</td>\n      <td>1.0</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>37</td>\n      <td>M</td>\n      <td>ATA</td>\n      <td>130</td>\n      <td>283</td>\n      <td>0</td>\n      <td>ST</td>\n      <td>98</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Up</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>48</td>\n      <td>F</td>\n      <td>ASY</td>\n      <td>138</td>\n      <td>214</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>108</td>\n      <td>Y</td>\n      <td>1.5</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>54</td>\n      <td>M</td>\n      <td>NAP</td>\n      <td>150</td>\n      <td>195</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>122</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Up</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>913</th>\n      <td>45</td>\n      <td>M</td>\n      <td>TA</td>\n      <td>110</td>\n      <td>264</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>132</td>\n      <td>N</td>\n      <td>1.2</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>914</th>\n      <td>68</td>\n      <td>M</td>\n      <td>ASY</td>\n      <td>144</td>\n      <td>193</td>\n      <td>1</td>\n      <td>Normal</td>\n      <td>141</td>\n      <td>N</td>\n      <td>3.4</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>915</th>\n      <td>57</td>\n      <td>M</td>\n      <td>ASY</td>\n      <td>130</td>\n      <td>131</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>115</td>\n      <td>Y</td>\n      <td>1.2</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>916</th>\n      <td>57</td>\n      <td>F</td>\n      <td>ATA</td>\n      <td>130</td>\n      <td>236</td>\n      <td>0</td>\n      <td>LVH</td>\n      <td>174</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>917</th>\n      <td>38</td>\n      <td>M</td>\n      <td>NAP</td>\n      <td>138</td>\n      <td>175</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>173</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Up</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>918 rows × 12 columns</p>\n</div>","text/plain":"     Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  \\\n0     40   M           ATA        140          289          0     Normal   \n1     49   F           NAP        160          180          0     Normal   \n2     37   M           ATA        130          283          0         ST   \n3     48   F           ASY        138          214          0     Normal   \n4     54   M           NAP        150          195          0     Normal   \n..   ...  ..           ...        ...          ...        ...        ...   \n913   45   M            TA        110          264          0     Normal   \n914   68   M           ASY        144          193          1     Normal   \n915   57   M           ASY        130          131          0     Normal   \n916   57   F           ATA        130          236          0        LVH   \n917   38   M           NAP        138          175          0     Normal   \n\n     MaxHR ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n0      172              N      0.0       Up             0  \n1      156              N      1.0     Flat             1  \n2       98              N      0.0       Up             0  \n3      108              Y      1.5     Flat             1  \n4      122              N      0.0       Up             0  \n..     ...            ...      ...      ...           ...  \n913    132              N      1.2     Flat             1  \n914    141              N      3.4     Flat             1  \n915    115              Y      1.2     Flat             1  \n916    174              N      0.0     Flat             1  \n917    173              N      0.0       Up             0  \n\n[918 rows x 12 columns]"},"exec_count":20,"output_type":"execute_result"}},"pos":2,"type":"cell"}
{"cell_type":"code","exec_count":21,"id":"f2ef38","input":"#heart_df['female'] = heart_df['female'].map({'F': 1, 'M': 0})\nheart_df['Sex'].replace('F',0 ,inplace=True)\nheart_df['Sex'].replace('M', 1,inplace=True)\n#Female is 0\n#Male is 1\n\n#ATA is 0\n#NAP is 1\n#ASY is 2\n#TA is 3\nheart_df['ChestPainType'].replace('ATA',0 ,inplace=True)\nheart_df['ChestPainType'].replace('NAP',1 ,inplace=True)\nheart_df['ChestPainType'].replace('ASY',2 ,inplace=True)\nheart_df['ChestPainType'].replace('TA',3 ,inplace=True)\n\n#Normal is 0\n#St is 1\n#LVH is 2\nheart_df['RestingECG'].replace('Normal',0, inplace=True)\nheart_df['RestingECG'].replace('ST',1, inplace=True)\nheart_df['RestingECG'].replace('LVH',2, inplace=True)\n\n#No is 0\n#Yes is 1\nheart_df['ExerciseAngina'].replace('N',0 ,inplace=True)\nheart_df['ExerciseAngina'].replace('Y',1 ,inplace=True)\n\n#ST_Slope\n#Up is 0\n#Flat is 1\n#Down is 2\nheart_df['ST_Slope'].replace('Up', 0, inplace = True)\nheart_df['ST_Slope'].replace('Flat', 1, inplace = True)\nheart_df['ST_Slope'].replace('Down', 2, inplace = True)\n\n#My computer just crashed, sorry!\n#np. Anthony is explaining scatter plots right now.\n\nheart_df.head() #worky :)","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>ChestPainType</th>\n      <th>RestingBP</th>\n      <th>Cholesterol</th>\n      <th>FastingBS</th>\n      <th>RestingECG</th>\n      <th>MaxHR</th>\n      <th>ExerciseAngina</th>\n      <th>Oldpeak</th>\n      <th>ST_Slope</th>\n      <th>HeartDisease</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>40</td>\n      <td>1</td>\n      <td>0</td>\n      <td>140</td>\n      <td>289</td>\n      <td>0</td>\n      <td>0</td>\n      <td>172</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>49</td>\n      <td>0</td>\n      <td>1</td>\n      <td>160</td>\n      <td>180</td>\n      <td>0</td>\n      <td>0</td>\n      <td>156</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>37</td>\n      <td>1</td>\n      <td>0</td>\n      <td>130</td>\n      <td>283</td>\n      <td>0</td>\n      <td>1</td>\n      <td>98</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>48</td>\n      <td>0</td>\n      <td>2</td>\n      <td>138</td>\n      <td>214</td>\n      <td>0</td>\n      <td>0</td>\n      <td>108</td>\n      <td>1</td>\n      <td>1.5</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>54</td>\n      <td>1</td>\n      <td>1</td>\n      <td>150</td>\n      <td>195</td>\n      <td>0</td>\n      <td>0</td>\n      <td>122</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   Age  Sex  ChestPainType  RestingBP  Cholesterol  FastingBS  RestingECG  \\\n0   40    1              0        140          289          0           0   \n1   49    0              1        160          180          0           0   \n2   37    1              0        130          283          0           1   \n3   48    0              2        138          214          0           0   \n4   54    1              1        150          195          0           0   \n\n   MaxHR  ExerciseAngina  Oldpeak  ST_Slope  HeartDisease  \n0    172               0      0.0         0             0  \n1    156               0      1.0         1             1  \n2     98               0      0.0         0             0  \n3    108               1      1.5         1             1  \n4    122               0      0.0         0             0  "},"exec_count":21,"output_type":"execute_result"}},"pos":3,"type":"cell"}
{"cell_type":"code","exec_count":22,"id":"f04687","input":"heart_df.dropna(inplace=True)\nheart_df.shape","output":{"0":{"data":{"text/plain":"(918, 12)"},"exec_count":22,"output_type":"execute_result"}},"pos":4,"type":"cell"}
{"cell_type":"code","exec_count":249,"id":"cfe551","input":"# SVC Model\n\n#configuring the data\ntarget = heart_df['HeartDisease']\ninput_columns = heart_df.loc[:, heart_df.columns != \"HeartDisease\"]\n\n#splitting the data\nx_train, x_test, y_train, y_test = train_test_split(input_columns, target, test_size=0.2, random_state = 45)\n\nx_train.shape","output":{"0":{"data":{"text/plain":"(734, 11)"},"exec_count":249,"output_type":"execute_result"}},"pos":23,"type":"cell"}
{"cell_type":"code","exec_count":259,"id":"1f6bde","input":"#predicting results\ny_hat = my_SVC_model.predict(x_test)","pos":25,"type":"cell"}
{"cell_type":"code","exec_count":260,"id":"be3779","input":"y_hat","output":{"0":{"data":{"text/plain":"array([1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,\n       1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n       0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n       0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n       1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n       0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n       0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n       1, 1, 1, 1, 0, 0, 0, 1])"},"exec_count":260,"output_type":"execute_result"}},"pos":26,"type":"cell"}
{"cell_type":"code","exec_count":261,"id":"a2715d","input":"np.array(y_test)","output":{"0":{"data":{"text/plain":"array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n       1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n       0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n       0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n       1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n       1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n       0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1,\n       1, 1, 1, 1, 0, 1, 0, 1])"},"exec_count":261,"output_type":"execute_result"}},"pos":27,"type":"cell"}
{"cell_type":"code","exec_count":264,"id":"b814ee","input":"from sklearn.svm import SVC\n\n#changing the kernel\nmy_SVC_model = SVC(kernel = 'rbf')\n\n#fitting the model\nmy_SVC_model.fit(x_train, y_train)","output":{"0":{"data":{"text/html":"<style>#sk-container-id-16 {color: black;background-color: white;}#sk-container-id-16 pre{padding: 0;}#sk-container-id-16 div.sk-toggleable {background-color: white;}#sk-container-id-16 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-16 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-16 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-16 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-16 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-16 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-16 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-16 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-16 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-16 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-16 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-16 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-16 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-16 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-16 div.sk-item {position: relative;z-index: 1;}#sk-container-id-16 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-16 div.sk-item::before, #sk-container-id-16 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-16 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-16 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-16 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-16 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-16 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-16 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-16 div.sk-label-container {text-align: center;}#sk-container-id-16 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-16 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-16\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" checked><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>","text/plain":"SVC()"},"exec_count":264,"output_type":"execute_result"}},"pos":24,"type":"cell"}
{"cell_type":"code","exec_count":265,"id":"e8d731","input":"#MSE\ntotal_squared_error = (np.sum((y_test - y_hat)**2)) #get the sum of all the errors (error = what we want (y_test) - what we predicted (y_hat))\nmean_squared_error = total_squared_error/len(y_test) #divide this by how many rows/observations we have \nprint(mean_squared_error)\n#kernel on poly degree 3 gave .2771 /80/20\n#kernel on linear gave .1467 /80/20\n#kernel on rbf gave .1467 /80/20\n#kernel on sigmoid gave  /80/20\n\n#kernel on poly degree 3 gave  /70/30\n#kernel on linear gave  /70/30\n#kernel on rbf gave  /70/30\n#kernel on sigmoid gave  /70/30","output":{"0":{"name":"stdout","output_type":"stream","text":"0.14673913043478262\n"}},"pos":28,"type":"cell"}
{"cell_type":"code","exec_count":266,"id":"26cd3d","input":"#Confusion Matrix\nsns.heatmap(confusion_matrix(y_test, y_hat), annot=True, fmt='g')\nacc = accuracy_score(y_test, y_hat)\nfone_score = f1_score(y_test, y_hat)\nrec = recall_score(y_test, y_hat)\nprec = precision_score(y_test, y_hat)\n\n#accuracy, f1 score, recall, precision\nprint(\"Accuracy: \" , acc , \"f1 Score: \" , fone_score, \"Recall: \" , rec, \"Precision: \" , prec)\n#kernel on poly degree 3 gave .7228, .7411, .7849, .7019 /80/20\n#kernel on linear gave .8532, .8601, .8924, .83 /80/20\n#kernel on rbf gave  /80/20\n#kernel on sigmoid gave  /80/20\n\n#kernel on poly degree 3 gave  /70/30\n#kernel on linear gave  /70/30\n#kernel on rbf gave  /70/30\n#kernel on sigmoid gave  /70/30","output":{"0":{"name":"stdout","output_type":"stream","text":"Accuracy:  0.8532608695652174 f1 Score:  0.8601036269430051 Recall:  0.8924731182795699 Precision:  0.83\n"},"1":{"data":{"image/png":"f07bf6d88f4176f85f4ff20d1d4eacf8fec22207","text/plain":"<Figure size 432x288 with 2 Axes>"},"exec_count":266,"metadata":{"needs_background":"light"},"output_type":"execute_result"}},"pos":29,"type":"cell"}
{"cell_type":"code","exec_count":269,"id":"7a3683","input":"heart_dfs = heart_df.copy()\ntarget = heart_dfs[\"HeartDisease\"]\ninput_columns = heart_dfs.loc[:, heart_dfs.columns != \"HeartDisease\"]\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\n\n\n\nx_train, x_test, y_train, y_test = train_test_split(input_columns, target, test_size=0.2,stratify =y, random_state=42)\nKNN_model = KNN(n_neighbors = 7)\nKNN_model.fit(x_train, y_train)\ny_hat = KNN_model.predict(x_test)\n\ntotal_squared_error = (np.sum((y_test - y_hat)**2)) #get the sum of all the errors (error = what we want (y_test) - what we predicted (y_hat))\nmean_squared_error = total_squared_error/len(y_test) #divide this by how many rows/observations we have \n#print(mean_squared_error)\n\n\nsns.heatmap(confusion_matrix(y_test, y_hat), annot=True, fmt='g')\nacc = accuracy_score(y_test, y_hat)\nprec = precision_score(y_test, y_hat)\nrecall = recall_score(y_test, y_hat)\nf1 = f1_score(y_test, y_hat)\nscores['KNN'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}\nprint(scores['KNN'])","output":{"0":{"name":"stdout","output_type":"stream","text":"{'accuracy': 0.7010869565217391, 'precision': 0.719626168224299, 'recall': 0.7549019607843137, 'f1_score': 0.736842105263158}\n"},"1":{"data":{"image/png":"5f40de92d9e2c7cd448f380e0f00f3251a2193aa","text/plain":"<Figure size 432x288 with 2 Axes>"},"exec_count":269,"metadata":{"needs_background":"light"},"output_type":"execute_result"}},"pos":12,"type":"cell"}
{"cell_type":"code","exec_count":270,"id":"ded656","input":"#print(y_hat)\nprint(np.array(y_test))\n\n\"\"\"total_squared_error = (np.sum((y_test - y_hat)**2)) #get the sum of all the errors (error = what we want (y_test) - what we predicted (y_hat))\nmean_squared_error = total_squared_error/len(y_test) #divide this by how many rows/observations we have \nprint(mean_squared_error)\"\"\"\n\"\"\"sns.heatmap(confusion_matrix(y_test, y_hat), annot=True, fmt='g')\nacc = accuracy_score(y_test, y_hat)\nprec = precision_score(y_test, y_hat, average='micro')\nrecall = recall_score(y_test, y_hat, average='micro')\nf1 = f1_score(y_test, y_hat, average='micro')\nscores['KNN'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}\nprint(scores['KNN'])\"\"\"","output":{"0":{"name":"stdout","output_type":"stream","text":"[1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1\n 1 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1\n 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1\n 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0\n 1 1 1 0 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1]\n"},"1":{"data":{"text/plain":"\"sns.heatmap(confusion_matrix(y_test, y_hat), annot=True, fmt='g')\\nacc = accuracy_score(y_test, y_hat)\\nprec = precision_score(y_test, y_hat, average='micro')\\nrecall = recall_score(y_test, y_hat, average='micro')\\nf1 = f1_score(y_test, y_hat, average='micro')\\nscores['KNN'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}\\nprint(scores['KNN'])\""},"exec_count":270,"output_type":"execute_result"}},"pos":14,"type":"cell"}
{"cell_type":"code","exec_count":272,"id":"33efbc","input":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport numpy\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns\nimport sklearn\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import roc_auc_score","pos":0,"type":"cell"}
{"cell_type":"code","exec_count":273,"id":"f72a8a","input":"#x is everything but heart disease\nx = heart_df.loc[:, heart_df.columns != \"HeartDisease\"]\n#y is heart disease\ny = heart_df['HeartDisease']\n# splits dataset; 80 percent train: 20 percent test\nx_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.2, random_state=0)\n#scales down the x values\nst_x= StandardScaler()\nx_train= st_x.fit_transform(x_train)\nx_test= st_x.transform(x_test)\nclassifier = LogisticRegression(random_state=42)\n#Fits dataset\nclassifier.fit(x_train, y_train)\n#gets predicted values\ny_pred= classifier.predict(x_test)\n#compares predictions to actual values\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\nprint(cm)\n\"\"\"total_squared_error = (np.sum((y_test - y_pred)**2))\nmean_squared_error = total_squared_error/len(y_test)\nprint(mean_squared_error)\"\"\"\n\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\nscores['LOGISTICREGRESSION'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}\nprint(scores['LOGISTICREGRESSION'])\n\n","output":{"0":{"name":"stdout","output_type":"stream","text":"[[60 11]\n [17 96]]\n{'accuracy': 0.8478260869565217, 'precision': 0.8495575221238938, 'recall': 0.897196261682243, 'f1_score': 0.8727272727272727}\n"},"1":{"data":{"image/png":"0dc2221b6f39dd0fd704be0c36c39e9b414dee61","text/plain":"<Figure size 432x288 with 3 Axes>"},"exec_count":273,"metadata":{"needs_background":"light"},"output_type":"execute_result"}},"pos":17,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":278,"id":"5679c3","input":"y_pred_proba = classifier.predict_proba(x_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"heart dataframe, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\n\ny = label_binarize(y, classes=[0, 1, 2])\nn_classes = y.shape[1]\n\n\n\n\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\nplt.figure()\nlw = 2\nplt.plot(\n    fpr[2],\n    tpr[2],\n    color=\"darkorange\",\n    lw=lw,\n    label=\"ROC curve (area = %0.2f)\" % roc_auc[2],\n)\nplt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver operating characteristic example\")\nplt.legend(loc=\"lower right\")\nplt.show()\n\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n#This plots the tradeoff of positive rate to false positive rate(roc curve)\n\"\"\"fitted_y = np.array(y_pred)\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(heart_df.loc[:, heart_df.columns != \"HeartDisease\"],heart_df['HeartDisease'],c='red', marker='o', alpha=0.5)\nax.plot_surface(x_test,y_test,y_pred.reshape(x_test.shape), color='b', alpha=0.3)\nax.set_xlabel('Price')\nax.set_ylabel('AdSpends')\nax.set_zlabel('Sales')\nplt.show()\"\"\"","output":{"0":{"data":{"image/png":"5fa8477356f0c0f9dabcf98ef852006c23c12eea","text/plain":"<Figure size 432x288 with 1 Axes>"},"exec_count":278,"metadata":{"needs_background":"light"},"output_type":"execute_result"},"1":{"ename":"KeyError","evalue":"'key of type tuple not found and not a MultiIndex'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Input \u001b[0;32mIn [278]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_classes):\n\u001b[0;32m---> 18\u001b[0m     fpr[i], tpr[i], _ \u001b[38;5;241m=\u001b[39m roc_curve(\u001b[43my_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m, y_pred[:, i])\n\u001b[1;32m     19\u001b[0m     roc_auc[i] \u001b[38;5;241m=\u001b[39m auc(fpr[i], tpr[i])\n\u001b[1;32m     20\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n","File \u001b[0;32m~/miniconda3/envs/ds_env/lib/python3.8/site-packages/pandas/core/series.py:984\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    981\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_values(key)\n\u001b[0;32m--> 984\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/miniconda3/envs/ds_env/lib/python3.8/site-packages/pandas/core/series.py:999\u001b[0m, in \u001b[0;36mSeries._get_with\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    994\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    995\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexing a Series with DataFrame is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    996\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported, use the appropriate DataFrame column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    997\u001b[0m     )\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 999\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_values_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(key):\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;66;03m# e.g. scalars that aren't recognized by lib.is_scalar, GH#32684\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc[key]\n","File \u001b[0;32m~/miniconda3/envs/ds_env/lib/python3.8/site-packages/pandas/core/series.py:1034\u001b[0m, in \u001b[0;36mSeries._get_values_tuple\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n\u001b[0;32m-> 1034\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey of type tuple not found and not a MultiIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m# If key is contained, would have returned by now\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m indexer, new_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc_level(key)\n","\u001b[0;31mKeyError\u001b[0m: 'key of type tuple not found and not a MultiIndex'"]}},"pos":18,"type":"cell"}
{"cell_type":"code","exec_count":43,"id":"5715dd","input":"scores = {}\nscores['metrics'] = {'accuracy': 'accuracy', 'precision':'precision', 'recall':'recall', 'f1_score':'f1_score'}","pos":7,"type":"cell"}
{"cell_type":"code","exec_count":46,"id":"d8b80d","input":"# Please save scores like the example below\n        scores['knn'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}","pos":16,"type":"cell"}
{"cell_type":"code","exec_count":56,"id":"c1e1c5","input":"","output":{"0":{"data":{"text/plain":"184"},"exec_count":56,"output_type":"execute_result"}},"pos":22,"type":"cell"}
{"cell_type":"code","exec_count":64,"id":"6c1a32","input":"(89)/(89+12)\n(65)/(65+18)","output":{"0":{"data":{"text/plain":"0.7831325301204819"},"exec_count":64,"output_type":"execute_result"}},"pos":21,"type":"cell"}
{"cell_type":"code","exec_count":74,"id":"00f5ec","input":"\"\"\"target = heart_df[\"HeartDisease\"]\ninput_columns = heart_df.loc[:, heart_df.columns != \"HeartDisease\"]\nx_train, x_test, y_train, y_test = train_test_split(input_columns, target, test_size=0.3)\"\"\"","pos":8,"type":"cell"}
{"cell_type":"code","exec_count":75,"id":"c6494e","input":"x_train.shape","output":{"0":{"data":{"text/plain":"(642, 11)"},"exec_count":75,"output_type":"execute_result"}},"pos":9,"type":"cell"}
{"cell_type":"code","exec_count":78,"id":"ef2113","input":"#ADABOOST\ntop_scores = [{'model': 0, 'mean_squared_error': 0}, {'model': 0, 'acc': 0}, {'model': 0, 'prec': 0}, {'model': 0, 'recall': 0}, {'model': 0, 'f1': 0}]\n    \nfor i in range(100, 201):\n    abc = AdaBoostClassifier(n_estimators=i)\n    abc.fit(x_train, y_train)\n    y_predictions = abc.predict(x_test)\n    \n    total_squared_error = (np.sum((y_test - y_predictions)**2)) #get the sum of all the errors (error = what we want (y_test) - what we predicted (y_hat))\n    mean_squared_error = total_squared_error/len(y_test) #divide this by how many rows/observations we have \n    acc = accuracy_score(y_test, y_predictions)\n    prec = precision_score(y_test, y_predictions)\n    recall = recall_score(y_test, y_predictions)\n    f1 = f1_score(y_test, y_predictions)\n    \n    results = [mean_squared_error, acc, prec, recall, f1]\n    \n    if top_scores[0]['mean_squared_error'] < results[0]:\n        top_scores[0]['mean_squared_error'] = results[0]\n        top_scores[0]['model'] = i\n    if top_scores[1]['acc'] < results[1]:\n        top_scores[1]['acc'] = results[1]\n        top_scores[1]['model'] = i\n    if top_scores[2]['prec'] < results[2]:\n        top_scores[2]['prec'] = results[2]\n        top_scores[2]['model'] = i\n    if top_scores[3]['recall'] < results[3]:\n        top_scores[3]['recall'] = results[3]\n        top_scores[3]['model'] = i\n    if top_scores[4]['f1'] < results[4]:\n        top_scores[4]['f1'] = results[4]\n        top_scores[4]['model'] = i\n    \n    \nprint(top_scores)\n\n\n# total_squared_error = (np.sum((y_test - y_predictions)**2)) #get the sum of all the errors (error = what we want (y_test) - what we predicted (y_hat))\n# mean_squared_error = total_squared_error/len(y_test) #divide this by how many rows/observations we have \n# print(mean_squared_error)\n# sns.heatmap(confusion_matrix(y_test, y_predictions), annot=True, fmt='g')\n# acc = accuracy_score(y_test, y_predictions)\n# prec = precision_score(y_test, y_predictions, average='micro')\n# recall = recall_score(y_test, y_predictions, average='micro')\n# f1 = f1_score(y_test, y_predictions, average='micro')\n# scores['Adaboost'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}\n# print(scores['Adaboost'])","output":{"0":{"name":"stdout","output_type":"stream","text":"[{'model': 183, 'mean_squared_error': 0.19927536231884058}, {'model': 103, 'acc': 0.8478260869565217}, {'model': 103, 'prec': 0.8478260869565217}, {'model': 103, 'recall': 0.8478260869565217}, {'model': 103, 'f1': 0.8478260869565218}]\n"}},"pos":11,"type":"cell"}
{"cell_type":"code","exec_count":85,"id":"57885a","input":"np.arange(60,100, 5)","output":{"0":{"data":{"text/plain":"array([60, 65, 70, 75, 80, 85, 90, 95])"},"exec_count":85,"output_type":"execute_result"}},"pos":1,"type":"cell"}
{"cell_type":"markdown","id":"1de317","input":"grid search cv","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"d38595","input":"**NOTE:** PLEASE save the scores to the score dictionary \n\nModels to try out:\n\n- Decision Tree \\(Emma\\)\n- Random Forest\n- NEURAL NETWORKS  Thomas\n- \n- Logistic Regression\n- \n- Naive Bayes \\(Jeffrey and Rhone\\)\n- Adaboost\n- Support vector classifier\n- \n- \n- Support vector classifier \n- KNN\n\n<u>**AlSO TRY OPTIMIZING THEM**</u>\n\nGrid Search CV\n\nEvaluation metrics:\n\n- F1 score\n- Accuracy\n- Recall\n- Precision\n- Confusion matrix\n\n","pos":5,"type":"cell"}
{"id":0,"time":1658859561693,"type":"user"}
{"last_load":1658868848238,"type":"file"}