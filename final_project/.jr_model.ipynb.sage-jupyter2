{"backend_state":"running","connection_file":"/projects/d5a3aa23-997d-4ffc-977a-1dc20c583e62/.local/share/jupyter/runtime/kernel-4247fa98-b260-4021-a236-2a464760c7ad.json","kernel":"ds_env","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1659625383985,"exec_count":35,"id":"cfe551","input":"# SVC Model\n\n#configuring the data\ntarget = heart_df['HeartDisease']\ninput_columns = heart_df.loc[:, heart_df.columns != \"HeartDisease\"]\n\n#splitting the data\nx_train, x_test, y_train, y_test = train_test_split(input_columns, target, test_size=0.2, random_state = 2)\n\nx_train.shape","kernel":"ds_env","output":{"0":{"data":{"text/plain":"(734, 11)"},"exec_count":35}},"pos":35,"start":1659625383968,"state":"done","type":"cell"}
{"cell_type":"code","end":1659625385681,"exec_count":36,"id":"b814ee","input":"from sklearn.svm import SVC\n\n#changing the kernel\nmy_SVC_model = SVC(kernel = 'rbf')\n\n#fitting the model\nmy_SVC_model.fit(x_train, y_train)","kernel":"ds_env","output":{"0":{"data":{"text/html":"<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>","text/plain":"SVC()"},"exec_count":36}},"pos":36,"start":1659625385639,"state":"done","type":"cell"}
{"cell_type":"code","end":1659625386403,"exec_count":37,"id":"1f6bde","input":"#predicting results\ny_hat = my_SVC_model.predict(x_test)","kernel":"ds_env","pos":37,"start":1659625386393,"state":"done","type":"cell"}
{"cell_type":"code","end":1659625387128,"exec_count":38,"id":"be3779","input":"y_hat","kernel":"ds_env","output":{"0":{"data":{"text/plain":"array([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n       0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n       1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n       1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n       0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n       1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n       0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n       1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n       0, 1, 1, 1, 1, 0, 1, 0])"},"exec_count":38}},"pos":38,"start":1659625387117,"state":"done","type":"cell"}
{"cell_type":"code","end":1659625387827,"exec_count":39,"id":"a2715d","input":"np.array(y_test)","kernel":"ds_env","output":{"0":{"data":{"text/plain":"array([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n       0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n       1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n       0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n       1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n       0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n       0, 0, 1, 0, 1, 1, 1, 0])"},"exec_count":39}},"pos":39,"start":1659625387822,"state":"done","type":"cell"}
{"cell_type":"code","end":1659625388479,"exec_count":40,"id":"e8d731","input":"\ntotal_squared_error = (np.sum((y_test - y_hat)**2)) #get the sum of all the errors (error = what we want (y_test) - what we predicted (y_hat))\nmean_squared_error = total_squared_error/len(y_test) #divide this by how many rows/observations we have \nprint(mean_squared_error)","kernel":"ds_env","output":{"0":{"name":"stdout","text":"0.2608695652173913\n"}},"pos":40,"start":1659625388476,"state":"done","type":"cell"}
{"cell_type":"code","end":1659625389808,"exec_count":41,"id":"26cd3d","input":"#Confusion Matrix\nsns.heatmap(confusion_matrix(y_test, y_hat), annot=True, fmt='g')\nacc = accuracy_score(y_test, y_hat)\nfone_score = f1_score(y_test, y_hat)\nrec = recall_score(y_test, y_hat)\nprec = precision_score(y_test, y_hat)\n\n\nprint(\"Accuracy: \" , acc , \"f1_Score: \" , fone_score, \"Recall: \" , prec, \"Precision: \" , prec)","kernel":"ds_env","output":{"0":{"name":"stdout","text":"Accuracy:  0.7391304347826086 f1_Score:  0.7525773195876289 Recall:  0.7373737373737373 Precision:  0.7373737373737373\n"},"1":{"data":{"image/png":"92deb77f90b0c0e0e07aec592b85a784b0b74b6d","text/plain":"<Figure size 432x288 with 2 Axes>"},"metadata":{"needs_background":"light"}}},"pos":41,"start":1659625389632,"state":"done","type":"cell"}
{"cell_type":"code","end":1659625394416,"exec_count":42,"id":"0dff3f","input":"x_train","kernel":"ds_env","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>ChestPainType</th>\n      <th>RestingBP</th>\n      <th>Cholesterol</th>\n      <th>FastingBS</th>\n      <th>RestingECG</th>\n      <th>MaxHR</th>\n      <th>ExerciseAngina</th>\n      <th>Oldpeak</th>\n      <th>ST_Slope</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>400</th>\n      <td>50</td>\n      <td>0</td>\n      <td>2</td>\n      <td>160</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>110</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>107</th>\n      <td>34</td>\n      <td>1</td>\n      <td>0</td>\n      <td>150</td>\n      <td>214</td>\n      <td>0</td>\n      <td>1</td>\n      <td>168</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>565</th>\n      <td>61</td>\n      <td>1</td>\n      <td>2</td>\n      <td>141</td>\n      <td>292</td>\n      <td>0</td>\n      <td>1</td>\n      <td>115</td>\n      <td>1</td>\n      <td>1.7</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>578</th>\n      <td>57</td>\n      <td>1</td>\n      <td>2</td>\n      <td>156</td>\n      <td>173</td>\n      <td>0</td>\n      <td>2</td>\n      <td>119</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>372</th>\n      <td>63</td>\n      <td>1</td>\n      <td>2</td>\n      <td>185</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>98</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>534</th>\n      <td>59</td>\n      <td>1</td>\n      <td>2</td>\n      <td>125</td>\n      <td>222</td>\n      <td>0</td>\n      <td>0</td>\n      <td>135</td>\n      <td>1</td>\n      <td>2.5</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>584</th>\n      <td>64</td>\n      <td>1</td>\n      <td>2</td>\n      <td>141</td>\n      <td>244</td>\n      <td>1</td>\n      <td>1</td>\n      <td>116</td>\n      <td>1</td>\n      <td>1.5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>493</th>\n      <td>51</td>\n      <td>1</td>\n      <td>1</td>\n      <td>137</td>\n      <td>339</td>\n      <td>0</td>\n      <td>0</td>\n      <td>127</td>\n      <td>1</td>\n      <td>1.7</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>527</th>\n      <td>61</td>\n      <td>1</td>\n      <td>0</td>\n      <td>139</td>\n      <td>283</td>\n      <td>0</td>\n      <td>0</td>\n      <td>135</td>\n      <td>0</td>\n      <td>0.3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>168</th>\n      <td>58</td>\n      <td>1</td>\n      <td>2</td>\n      <td>135</td>\n      <td>222</td>\n      <td>0</td>\n      <td>0</td>\n      <td>100</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>734 rows × 11 columns</p>\n</div>","text/plain":"     Age  Sex  ChestPainType  RestingBP  Cholesterol  FastingBS  RestingECG  \\\n400   50    0              2        160            0          1           0   \n107   34    1              0        150          214          0           1   \n565   61    1              2        141          292          0           1   \n578   57    1              2        156          173          0           2   \n372   63    1              2        185            0          0           0   \n..   ...  ...            ...        ...          ...        ...         ...   \n534   59    1              2        125          222          0           0   \n584   64    1              2        141          244          1           1   \n493   51    1              1        137          339          0           0   \n527   61    1              0        139          283          0           0   \n168   58    1              2        135          222          0           0   \n\n     MaxHR  ExerciseAngina  Oldpeak  ST_Slope  \n400    110               0      0.0         1  \n107    168               0      0.0         0  \n565    115               1      1.7         1  \n578    119               1      3.0         2  \n372     98               1      0.0         0  \n..     ...             ...      ...       ...  \n534    135               1      2.5         2  \n584    116               1      1.5         1  \n493    127               1      1.7         1  \n527    135               0      0.3         0  \n168    100               0      0.0         0  \n\n[734 rows x 11 columns]"},"exec_count":42}},"pos":42,"start":1659625394403,"state":"done","type":"cell"}
{"cell_type":"code","end":1659625484418,"exec_count":43,"id":"458d0b","input":"svc_clf = SVC()\nsvc_clf.fit(x_train, y_train)\nsvc_pred = svc_clf.predict(x_test)\nprec = precision_score(y_test, svc_pred)\nprint(prec)","kernel":"ds_env","output":{"0":{"name":"stdout","text":"0.7373737373737373\n"}},"pos":43,"start":1659625484376,"state":"done","type":"cell"}
{"cell_type":"code","end":1659625490186,"exec_count":44,"id":"d79a2c","input":"svc_pred","kernel":"ds_env","output":{"0":{"data":{"text/plain":"array([1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n       0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n       1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n       1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n       0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n       1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n       0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0,\n       1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n       0, 1, 1, 1, 1, 0, 1, 0])"},"exec_count":44}},"pos":44,"start":1659625490176,"state":"done","type":"cell"}
{"cell_type":"code","end":1659625555696,"exec_count":49,"id":"f76d80","input":"from sklearn.svm import SVC\n\n#changing the kernel\nmy_SVC_model = SVC(kernel = 'linear',)\n\n#fitting the model\nmy_SVC_model.fit(x_train, y_train)","kernel":"ds_env","output":{"0":{"data":{"text/html":"<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>","text/plain":"SVC(kernel='linear')"},"exec_count":49}},"pos":46,"start":1659625552674,"state":"done","type":"cell"}
{"cell_type":"code","end":1659625558108,"exec_count":50,"id":"f23984","input":"y_hat = my_SVC_model.predict(x_test)\ntotal_squared_error = (np.sum((y_test - y_hat)**2)) #get the sum of all the errors (error = what we want (y_test) - what we predicted (y_hat))\nmean_squared_error = total_squared_error/len(y_test) #divide this by how many rows/observations we have \nprint(mean_squared_error)","kernel":"ds_env","output":{"0":{"name":"stdout","text":"0.1358695652173913\n"}},"pos":47,"start":1659625558093,"state":"done","type":"cell"}
{"cell_type":"code","end":1659625593235,"exec_count":53,"id":"5e1f5c","input":"#Confusion Matrix\ntitle = \"SVC Confusion Matrix\"\nSVC, ax = plt.subplots(figsize=(10, 10))\ncm = confusion_matrix(y_test, y_hat, labels=my_SVC_model.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=my_SVC_model.classes_)\ndisp.plot(ax=ax)\ndisp.ax_.set_title(title)\nSVC.savefig(\"confusion_matrices/my_SVC_model.jpg\")\n#sns.heatmap(confusion_matrix(y_test, y_hat), annot=True, fmt='g')\nacc = accuracy_score(y_test, y_hat)\nfone_score = f1_score(y_test, y_hat)\nrec = recall_score(y_test, y_hat)\nprec = precision_score(y_test, y_hat)\n\n#accuracy, f1 score, recall, precision\nprint(\"Accuracy: \" , acc , \"f1 Score: \" , fone_score, \"Recall: \" , rec, \"Precision: \" , prec)","kernel":"ds_env","output":{"0":{"name":"stdout","text":"Accuracy:  0.8641304347826086 f1 Score:  0.8691099476439791 Recall:  0.8736842105263158 Precision:  0.8645833333333334\n"},"1":{"data":{"image/png":"12102ff9ef29cd20771524b5b74068e3eb3d222c","text/plain":"<Figure size 720x720 with 2 Axes>"},"metadata":{"needs_background":"light"}}},"pos":48,"start":1659625592982,"state":"done","type":"cell"}
{"cell_type":"code","end":1659627368576,"exec_count":55,"id":"33efbc","input":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport numpy\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns\nimport sklearn\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import neighbors, datasets\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay","kernel":"ds_env","pos":1,"start":1659627368555,"state":"done","type":"cell"}
{"cell_type":"code","end":1659627369312,"exec_count":56,"id":"57885a","input":"np.arange(60,100, 5)","kernel":"ds_env","output":{"0":{"data":{"text/plain":"array([60, 65, 70, 75, 80, 85, 90, 95])"},"exec_count":56}},"pos":2,"start":1659627369305,"state":"done","type":"cell"}
{"cell_type":"code","end":1659627370012,"exec_count":57,"id":"2a6364","input":"heart_df = pd.read_csv(\"data/heart.csv\")\nheart_df\n","kernel":"ds_env","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>ChestPainType</th>\n      <th>RestingBP</th>\n      <th>Cholesterol</th>\n      <th>FastingBS</th>\n      <th>RestingECG</th>\n      <th>MaxHR</th>\n      <th>ExerciseAngina</th>\n      <th>Oldpeak</th>\n      <th>ST_Slope</th>\n      <th>HeartDisease</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>40</td>\n      <td>M</td>\n      <td>ATA</td>\n      <td>140</td>\n      <td>289</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>172</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Up</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>49</td>\n      <td>F</td>\n      <td>NAP</td>\n      <td>160</td>\n      <td>180</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>156</td>\n      <td>N</td>\n      <td>1.0</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>37</td>\n      <td>M</td>\n      <td>ATA</td>\n      <td>130</td>\n      <td>283</td>\n      <td>0</td>\n      <td>ST</td>\n      <td>98</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Up</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>48</td>\n      <td>F</td>\n      <td>ASY</td>\n      <td>138</td>\n      <td>214</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>108</td>\n      <td>Y</td>\n      <td>1.5</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>54</td>\n      <td>M</td>\n      <td>NAP</td>\n      <td>150</td>\n      <td>195</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>122</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Up</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>913</th>\n      <td>45</td>\n      <td>M</td>\n      <td>TA</td>\n      <td>110</td>\n      <td>264</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>132</td>\n      <td>N</td>\n      <td>1.2</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>914</th>\n      <td>68</td>\n      <td>M</td>\n      <td>ASY</td>\n      <td>144</td>\n      <td>193</td>\n      <td>1</td>\n      <td>Normal</td>\n      <td>141</td>\n      <td>N</td>\n      <td>3.4</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>915</th>\n      <td>57</td>\n      <td>M</td>\n      <td>ASY</td>\n      <td>130</td>\n      <td>131</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>115</td>\n      <td>Y</td>\n      <td>1.2</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>916</th>\n      <td>57</td>\n      <td>F</td>\n      <td>ATA</td>\n      <td>130</td>\n      <td>236</td>\n      <td>0</td>\n      <td>LVH</td>\n      <td>174</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>917</th>\n      <td>38</td>\n      <td>M</td>\n      <td>NAP</td>\n      <td>138</td>\n      <td>175</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>173</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Up</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>918 rows × 12 columns</p>\n</div>","text/plain":"     Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  \\\n0     40   M           ATA        140          289          0     Normal   \n1     49   F           NAP        160          180          0     Normal   \n2     37   M           ATA        130          283          0         ST   \n3     48   F           ASY        138          214          0     Normal   \n4     54   M           NAP        150          195          0     Normal   \n..   ...  ..           ...        ...          ...        ...        ...   \n913   45   M            TA        110          264          0     Normal   \n914   68   M           ASY        144          193          1     Normal   \n915   57   M           ASY        130          131          0     Normal   \n916   57   F           ATA        130          236          0        LVH   \n917   38   M           NAP        138          175          0     Normal   \n\n     MaxHR ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n0      172              N      0.0       Up             0  \n1      156              N      1.0     Flat             1  \n2       98              N      0.0       Up             0  \n3      108              Y      1.5     Flat             1  \n4      122              N      0.0       Up             0  \n..     ...            ...      ...      ...           ...  \n913    132              N      1.2     Flat             1  \n914    141              N      3.4     Flat             1  \n915    115              Y      1.2     Flat             1  \n916    174              N      0.0     Flat             1  \n917    173              N      0.0       Up             0  \n\n[918 rows x 12 columns]"},"exec_count":57}},"pos":3,"start":1659627369955,"state":"done","type":"cell"}
{"cell_type":"code","end":1659627370596,"exec_count":58,"id":"f2ef38","input":"#heart_df['female'] = heart_df['female'].map({'F': 1, 'M': 0})\nheart_df['Sex'].replace('F',0 ,inplace=True)\nheart_df['Sex'].replace('M', 1,inplace=True)\n#Female is 0\n#Male is 1\n\n#ATA is 0\n#NAP is 1\n#ASY is 2\n#TA is 3\nheart_df['ChestPainType'].replace('ATA',0 ,inplace=True)\nheart_df['ChestPainType'].replace('NAP',1 ,inplace=True)\nheart_df['ChestPainType'].replace('ASY',2 ,inplace=True)\nheart_df['ChestPainType'].replace('TA',3 ,inplace=True)\n\n#Normal is 0\n#St is 1\n#LVH is 2\nheart_df['RestingECG'].replace('Normal',0, inplace=True)\nheart_df['RestingECG'].replace('ST',1, inplace=True)\nheart_df['RestingECG'].replace('LVH',2, inplace=True)\n\n#No is 0\n#Yes is 1\nheart_df['ExerciseAngina'].replace('N',0 ,inplace=True)\nheart_df['ExerciseAngina'].replace('Y',1 ,inplace=True)\n\n#ST_Slope\n#Up is 0\n#Flat is 1\n#Down is 2\nheart_df['ST_Slope'].replace('Up', 0, inplace = True)\nheart_df['ST_Slope'].replace('Flat', 1, inplace = True)\nheart_df['ST_Slope'].replace('Down', 2, inplace = True)\n\nheart_df.head() #worky :)","kernel":"ds_env","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>ChestPainType</th>\n      <th>RestingBP</th>\n      <th>Cholesterol</th>\n      <th>FastingBS</th>\n      <th>RestingECG</th>\n      <th>MaxHR</th>\n      <th>ExerciseAngina</th>\n      <th>Oldpeak</th>\n      <th>ST_Slope</th>\n      <th>HeartDisease</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>40</td>\n      <td>1</td>\n      <td>0</td>\n      <td>140</td>\n      <td>289</td>\n      <td>0</td>\n      <td>0</td>\n      <td>172</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>49</td>\n      <td>0</td>\n      <td>1</td>\n      <td>160</td>\n      <td>180</td>\n      <td>0</td>\n      <td>0</td>\n      <td>156</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>37</td>\n      <td>1</td>\n      <td>0</td>\n      <td>130</td>\n      <td>283</td>\n      <td>0</td>\n      <td>1</td>\n      <td>98</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>48</td>\n      <td>0</td>\n      <td>2</td>\n      <td>138</td>\n      <td>214</td>\n      <td>0</td>\n      <td>0</td>\n      <td>108</td>\n      <td>1</td>\n      <td>1.5</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>54</td>\n      <td>1</td>\n      <td>1</td>\n      <td>150</td>\n      <td>195</td>\n      <td>0</td>\n      <td>0</td>\n      <td>122</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   Age  Sex  ChestPainType  RestingBP  Cholesterol  FastingBS  RestingECG  \\\n0   40    1              0        140          289          0           0   \n1   49    0              1        160          180          0           0   \n2   37    1              0        130          283          0           1   \n3   48    0              2        138          214          0           0   \n4   54    1              1        150          195          0           0   \n\n   MaxHR  ExerciseAngina  Oldpeak  ST_Slope  HeartDisease  \n0    172               0      0.0         0             0  \n1    156               0      1.0         1             1  \n2     98               0      0.0         0             0  \n3    108               1      1.5         1             1  \n4    122               0      0.0         0             0  "},"exec_count":58}},"pos":4,"start":1659627370543,"state":"done","type":"cell"}
{"cell_type":"code","end":1659627371135,"exec_count":59,"id":"f04687","input":"heart_df.dropna(inplace=True)\nheart_df.shape","kernel":"ds_env","output":{"0":{"data":{"text/plain":"(918, 12)"},"exec_count":59}},"pos":5,"start":1659627371114,"state":"done","type":"cell"}
{"cell_type":"code","end":1659627372716,"exec_count":60,"id":"5715dd","input":"scores = {}\nscores['metrics'] = {'accuracy': 'accuracy', 'precision':'precision', 'recall':'recall', 'f1_score':'f1_score'}","kernel":"ds_env","pos":8,"start":1659627372706,"state":"done","type":"cell"}
{"cell_type":"code","end":1659627373203,"exec_count":61,"id":"0bf3c5","input":"x_test","kernel":"ds_env","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>ChestPainType</th>\n      <th>RestingBP</th>\n      <th>Cholesterol</th>\n      <th>FastingBS</th>\n      <th>RestingECG</th>\n      <th>MaxHR</th>\n      <th>ExerciseAngina</th>\n      <th>Oldpeak</th>\n      <th>ST_Slope</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>440</th>\n      <td>52</td>\n      <td>1</td>\n      <td>1</td>\n      <td>128</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>180</td>\n      <td>0</td>\n      <td>3.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>382</th>\n      <td>43</td>\n      <td>1</td>\n      <td>2</td>\n      <td>115</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>145</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>567</th>\n      <td>71</td>\n      <td>1</td>\n      <td>2</td>\n      <td>130</td>\n      <td>221</td>\n      <td>0</td>\n      <td>1</td>\n      <td>115</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>230</th>\n      <td>37</td>\n      <td>1</td>\n      <td>2</td>\n      <td>130</td>\n      <td>315</td>\n      <td>0</td>\n      <td>0</td>\n      <td>158</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>470</th>\n      <td>53</td>\n      <td>1</td>\n      <td>2</td>\n      <td>126</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>106</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>894</th>\n      <td>58</td>\n      <td>0</td>\n      <td>2</td>\n      <td>130</td>\n      <td>197</td>\n      <td>0</td>\n      <td>0</td>\n      <td>131</td>\n      <td>0</td>\n      <td>0.6</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>61</td>\n      <td>1</td>\n      <td>1</td>\n      <td>200</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>70</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>719</th>\n      <td>63</td>\n      <td>1</td>\n      <td>2</td>\n      <td>130</td>\n      <td>254</td>\n      <td>0</td>\n      <td>2</td>\n      <td>147</td>\n      <td>0</td>\n      <td>1.4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>318</th>\n      <td>61</td>\n      <td>1</td>\n      <td>2</td>\n      <td>130</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>77</td>\n      <td>0</td>\n      <td>2.5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>686</th>\n      <td>57</td>\n      <td>0</td>\n      <td>2</td>\n      <td>120</td>\n      <td>354</td>\n      <td>0</td>\n      <td>0</td>\n      <td>163</td>\n      <td>1</td>\n      <td>0.6</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>184 rows × 11 columns</p>\n</div>","text/plain":"     Age  Sex  ChestPainType  RestingBP  Cholesterol  FastingBS  RestingECG  \\\n440   52    1              1        128            0          0           1   \n382   43    1              2        115            0          0           0   \n567   71    1              2        130          221          0           1   \n230   37    1              2        130          315          0           0   \n470   53    1              2        126            0          0           0   \n..   ...  ...            ...        ...          ...        ...         ...   \n894   58    0              2        130          197          0           0   \n399   61    1              1        200            0          1           1   \n719   63    1              2        130          254          0           2   \n318   61    1              2        130            0          1           0   \n686   57    0              2        120          354          0           0   \n\n     MaxHR  ExerciseAngina  Oldpeak  ST_Slope  \n440    180               0      3.0         0  \n382    145               1      2.0         1  \n567    115               1      0.0         1  \n230    158               0      0.0         0  \n470    106               0      0.0         1  \n..     ...             ...      ...       ...  \n894    131               0      0.6         1  \n399     70               0      0.0         1  \n719    147               0      1.4         1  \n318     77               0      2.5         1  \n686    163               1      0.6         0  \n\n[184 rows x 11 columns]"},"exec_count":61}},"pos":9,"start":1659627373190,"state":"done","type":"cell"}
{"cell_type":"code","end":1659627373794,"exec_count":62,"id":"00f5ec","input":"target = heart_df[\"HeartDisease\"]\ninput_columns = heart_df.loc[:, heart_df.columns != \"HeartDisease\"]\nx_train, x_test, y_train, y_test = train_test_split(input_columns, target, test_size=0.3)\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","kernel":"ds_env","pos":10,"start":1659627373785,"state":"done","type":"cell"}
{"cell_type":"code","end":1659627374888,"exec_count":63,"id":"c6494e","input":"x_test.shape","kernel":"ds_env","output":{"0":{"data":{"text/plain":"(276, 11)"},"exec_count":63}},"pos":12,"start":1659627374880,"state":"done","type":"cell"}
{"cell_type":"code","end":1659627406121,"exec_count":67,"id":"ef2113","input":"#ADABOOST\n# parameters = {'n_estimators':[50, 100, 125, 150], 'learning_rate': [0.01, 0.1, 0.3, 0.5]}\n# scoring = ['precision', 'accuracy', 'recall', 'f1']\n\n\n# for score in scoring:\n#     abc = AdaBoostClassifier()\n#     grid_abc = GridSearchCV(abc, parameters, scoring=score)\n#     grid_abc.fit(x_train, y_train)\n\n\n#     print(\"best params for %s: \" % score)\n#     print(grid_abc.best_params_)\n\n#     means = grid_abc.cv_results_[\"mean_test_score\"]\n\n#     print(\"mean scores\")\n#     for mean, params in zip(means, grid_abc.cv_results_[\"params\"]):\n#         print(\"%0.5f for %r\" % (mean, params))\n\nada = AdaBoostClassifier(n_estimators=100, learning_rate=0.1)\nada.fit(x_train, y_train)\n\ny_predictions = ada.predict(x_test)\n\ntotal_squared_error = (np.sum((y_test - y_predictions)**2)) #get the sum of all the errors (error = what we want (y_test) - what we predicted (y_hat))\nmean_squared_error = total_squared_error/len(y_test) #divide this by how many rows/observations we have \n#print(mean_squared_error)\ntitle = \"Addaboost Confusion Matrix\"\nfig, ax = plt.subplots(figsize=(10, 10))\ncm = confusion_matrix(y_test, y_predictions, labels=ada.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=ada.classes_)\ndisp.plot(ax=ax)\ndisp.ax_.set_title(title)\nfig.savefig(\"confusion_matrices/ada.jpg\")\n\n\n\nacc = accuracy_score(y_test, y_predictions)\nprec = precision_score(y_test, y_predictions, average='micro')\nrecall = recall_score(y_test, y_predictions, average='micro')\nf1 = f1_score(y_test, y_predictions, average='micro')\nscores['Adaboost'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}\nprint(scores['Adaboost'])\n\n","kernel":"ds_env","output":{"0":{"name":"stdout","text":"{'accuracy': 0.8478260869565217, 'precision': 0.8478260869565217, 'recall': 0.8478260869565217, 'f1_score': 0.8478260869565218}\n"},"1":{"data":{"image/png":"6b9c752ec26d6e8d66a05ece2ee7382a0945bb9d","text/plain":"<Figure size 720x720 with 2 Axes>"},"metadata":{"needs_background":"light"}}},"pos":14,"scrolled":true,"start":1659627405659,"state":"done","type":"cell"}
{"cell_type":"code","end":1659633624024,"exec_count":69,"id":"9f2cce","input":"# define x as all columns but heart disease\n\nX = heart_df.loc[:, heart_df.columns != \"HeartDisease\"]\nX = (X - np.min(X)) / (np.max(X) - np.min(X))\n#define y as heart_disease\ny = heart_df['HeartDisease']\n#split data set 80 percent train: 20 percent test\n\n\nn_iter = 1000\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify = y)\n#scales down x_values\nst_x= StandardScaler()\nx_train= st_x.fit_transform(X_train)\nx_test= st_x.transform(X_test)\nthresholder = VarianceThreshold(threshold=.5)\n\nx_train = thresholder.fit_transform(x_train)\nx_test = thresholder.fit_transform(x_test)\n#create object model\ngnb = GaussianNB(priors=None, var_smoothing=1e-06)\n#fit object model\ngnb.fit(x_train, y_train)\nGaussianNB(priors=None, var_smoothing= 1)\ny_pred = gnb.predict(x_test)\n#print(\"Naive Bayes score: \",gnb.score(X_test, y_test))#\n#print(\"Number of mislabeled points out of a total %d points : %d\"\nfrom sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\ntitle = \"Naive Bayes Confusion Matrix\"\nfig, ax = plt.subplots(figsize=(10, 10))\ncm = confusion_matrix(y_test, y_pred, labels=gnb.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=gnb.classes_)\ndisp.plot(ax=ax)\ndisp.ax_.set_title(title)\nprint(fig)\nfig.savefig(\"confusion_matrices/gnb.jpg\")\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\nscores['NAIVEBAYES'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}\nprint(scores['NAIVEBAYES'])\n\nprint(\"Naive Bayes score: \",gnb.score(x_test, y_test))\n","kernel":"ds_env","output":{"0":{"name":"stderr","text":"/projects/d5a3aa23-997d-4ffc-977a-1dc20c583e62/miniconda3/envs/ds_env/lib/python3.8/site-packages/numpy/core/fromnumeric.py:84: FutureWarning: In a future version, DataFrame.min(axis=None) will return a scalar min over the entire DataFrame. To retain the old behavior, use 'frame.min(axis=0)' or just 'frame.min()'\n  return reduction(axis=axis, out=out, **passkwargs)\n/projects/d5a3aa23-997d-4ffc-977a-1dc20c583e62/miniconda3/envs/ds_env/lib/python3.8/site-packages/numpy/core/fromnumeric.py:84: FutureWarning: In a future version, DataFrame.max(axis=None) will return a scalar max over the entire DataFrame. To retain the old behavior, use 'frame.max(axis=0)' or just 'frame.max()'\n  return reduction(axis=axis, out=out, **passkwargs)\n"},"1":{"name":"stdout","text":"Figure(720x720)\n{'accuracy': 0.8858695652173914, 'precision': 0.8785046728971962, 'recall': 0.9215686274509803, 'f1_score': 0.8995215311004785}\nNaive Bayes score:  0.8858695652173914\n"},"2":{"data":{"image/png":"98e386ce5cc1418632136c59924a79a44b9c1b18","text/plain":"<Figure size 720x720 with 3 Axes>"},"metadata":{"needs_background":"light"}}},"pos":31,"start":1659633623560,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"2869ad","input":"#from sklearn.model_selection import GridSearchCV\n#from sklearn.svm import SVC as svc\n#from sklearn import preprocessing\n#from scipy import stats\n\nmdl = svc()\nparam_grid = {\"C\": [.1, 1, 10, 100, 1000], 'gamma': [.1, 1, 10, 100, 1000], 'kernel': ['poly', 'linear', 'rbf', 'sigmoid']}\ngrid = GridSearchCV(mdl, param_grid, refit = True, verbose = 7)\ngrid.fit(x_train, y_train)\n\n\ngrid_search = GridSearchCV(mdl, param_grid = grid_list, cv = 5)\ngrid_search.fit(x_train, y_train)\nprint(grid.best_params_)\n#print(\"hi\")","pos":49,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"4bb01a","input":"heart_dfs = heart_df.copy()\ntarget = heart_dfs[\"HeartDisease\"]\ninput_columns = heart_dfs.loc[:, heart_dfs.columns != \"HeartDisease\"]\ny = heart_df[\"HeartDisease\"]\nparam_grid = {'n_neighbors': range(1, 400)}\n\nx = KNN()\ngrid_x = GridSearchCV(x, param_grid)\nx_train, x_test, y_train, y_test = train_test_split(input_columns, target, test_size=0.2,stratify =y, random_state=42)\n\ngrid_x.fit(x_train, y_train)\nprint(grid_x.best_params_)","pos":16,"scrolled":true,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"5679c3","input":"y_pred_proba = classifier.predict_proba(x_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"heart dataframe, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()\n\"\"\"fpr = dict()\ntpr = dict()\nroc_auc = dict()\n\ny = label_binarize(y, classes=[0, 1, 2,])\nn_classes = y.shape[1]\n\n\nfor i in y_test:\n    i = int(i)\ny_test = list(y_test)\nfor i in y_pred:\n    i = int(i)\ny_pred = list(y_pred)\nprint(type(y_test))\nfor i in y_pred:\n    for x in y_test:\n        if i == -1 <= x:\n            print(i)\n\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[0:i], y_pred[0:i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\nplt.figure()\nlw = 2\nplt.plot(\n    fpr[2],\n    tpr[2],\n    color=\"darkorange\",\n    lw=lw,\n    label=\"ROC curve (area = %0.2f)\" % roc_auc[2],\n)\nplt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver operating characteristic example\")\nplt.legend(loc=\"lower right\")\nplt.show()\n\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n#This plots the tradeoff of positive rate to false positive rate(roc curve)\"\"\"\n\"\"\"fitted_y = np.array(y_pred)\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(heart_df.loc[:, heart_df.columns != \"HeartDisease\"],heart_df['HeartDisease'],c='red', marker='o', alpha=0.5)\nax.plot_surface(x_test,y_test,y_pred.reshape(x_test.shape), color='b', alpha=0.3)\nax.set_xlabel('Price')\nax.set_ylabel('AdSpends')\nax.set_zlabel('Sales')\nplt.show()\"\"\"","pos":29,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"605396","input":"\"\"\"#y_hat = KNN_model.predict(x_test)\nmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#00AAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])\n\nclf = neighbors.KNeighborsClassifier(n_neighbors = 23, weights='distance')\nclf.fit(x_train, y_train)\n\nx_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\ny_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\"\"\"","pos":19,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"6c1a32","input":"(89)/(89+12)\n(65)/(65+18)","pos":33,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"7a3683","input":"heart_dfs = heart_df.copy()\ntarget = heart_dfs[\"HeartDisease\"]\ninput_columns = heart_dfs.loc[:, heart_dfs.columns != \"HeartDisease\"]\n#sc = StandardScaler()\n#x_train = sc.fit_transform(x_train)\n#x_test = sc.transform(x_test)\n\n\n\n\nx_train, x_test, y_train, y_test = train_test_split(input_columns, target, test_size=0.2,stratify =target, random_state=42)\nKNN_OLD_MODEL = KNN()\nKNN_model = KNN(n_neighbors = 23)\nKNN_model.fit(x_train,y_train)\n\ny_hat = KNN_model.predict(x_test)\n\ntotal_squared_error = (np.sum((y_test - y_hat)**2)) #get the sum of all the errors (error = what we want (y_test) - what we predicted (y_hat))\nmean_squared_error = total_squared_error/len(y_test) #divide this by how many rows/observations we have \nprint(mean_squared_error)\n\n\nsns.heatmap(confusion_matrix(y_test, y_hat), annot=True, fmt='g')\nacc = accuracy_score(y_test, y_hat)\nprec = precision_score(y_test, y_hat)\nrecall = recall_score(y_test, y_hat)\nf1 = f1_score(y_test, y_hat)\nscores['KNN_ADJUSTED'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}\nprint(scores['KNN_ADJUSTED'])","pos":17,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"8ab8da","input":"lr_clf.intercept_","pos":27,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"ae328d","input":"","pos":0,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"b0ceee","input":"heart_df.columns","pos":25,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"c1e1c5","input":"","pos":34,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"d8b80d","input":"# Please save scores like the example below\nscores['knn'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}","pos":22,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"ded656","input":"#print(y_hat)\nprint(np.array(y_test))\n\n\"\"\"total_squared_error = (np.sum((y_test - y_hat)**2)) #get the sum of all the errors (error = what we want (y_test) - what we predicted (y_hat))\nmean_squared_error = total_squared_error/len(y_test) #divide this by how many rows/observations we have \nprint(mean_squared_error)\"\"\"\n\"\"\"sns.heatmap(confusion_matrix(y_test, y_hat), annot=True, fmt='g')\nacc = accuracy_score(y_test, y_hat)\nprec = precision_score(y_test, y_hat, average='micro')\nrecall = recall_score(y_test, y_hat, average='micro')\nf1 = f1_score(y_test, y_hat, average='micro')\nscores['KNN'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}\nprint(scores['KNN'])\"\"\"","pos":20,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"e7eb4b","input":"mlp = MLPClassifier(solver='lbfgs',hidden_layer_sizes=(1000, 35), alpha=0.05, max_iter=200, random_state = 1, activation='relu', learning_rate='adaptive')\nmlp.fit(x_train, y_train)\ny_hat = mlp.predict(x_test)\nprint(y_hat)\nprint(np.array(y_test))\ntotal_squared_error = (np.sum((y_test - y_hat)**2)) #get the sum of all the errors (error = what we want (y_test) - what we predicted (y_hat))\nmean_squared_error = total_squared_error/len(y_test) #divide this by how many rows/observations we have \nprint(mean_squared_error)\nsns.heatmap(confusion_matrix(y_test, y_hat), annot=True, fmt='g')\nacc = accuracy_score(y_test, y_hat)\nprec = precision_score(y_test, y_hat, average='micro')\nrecall = recall_score(y_test, y_hat, average='micro')\nf1 = f1_score(y_test, y_hat, average='micro')\nscores['mlp'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}\nprint(scores['mlp'])","pos":21,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"eb41fc","input":"heart_dfs = heart_df.copy()\ntarget = heart_dfs[\"HeartDisease\"]\ninput_columns = heart_dfs.loc[:, heart_dfs.columns != \"HeartDisease\"]\ny = heart_df[\"HeartDisease\"]\nparam_grid = {'var_smoothing':range(0,200)}\nx = GaussianNB()\ngrid_x = GridSearchCV(x, param_grid)\nx_train, x_test, y_train, y_test = train_test_split(input_columns, target, test_size=0.2,stratify =y, random_state=42)\n\ngrid_x.fit(x_train, y_train)\nprint(grid_x.best_params_)","pos":30,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"ec9934","input":"lr_clf.coef_","pos":26,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"ee5f57","input":"","pos":51,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"f72a8a","input":"#x is everything but heart disease\nx = heart_df.loc[:, heart_df.columns != \"HeartDisease\"]\n#y is heart disease\ny = heart_df['HeartDisease']\n# splits dataset; 80 percent train: 20 percent test\nx_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.2, random_state=0)\n#scales down the x values\nst_x= StandardScaler()\nx_train= st_x.fit_transform(x_train)\nx_test= st_x.transform(x_test)\nlr_clf = LogisticRegression(random_state=42)\n#Fits dataset\nlr_clf.fit(x_train, y_train)\n#gets predicted values\ny_pred= lr_clf.predict(x_test)\n#compares predictions to actual values\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\n\n\"\"\"total_squared_error = (np.sum((y_test - y_pred)**2))\nmean_squared_error = total_squared_error/len(y_test)\nprint(mean_squared_error)\"\"\"\n\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\nscores['LOGISTICREGRESSION'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}\nprint(scores['LOGISTICREGRESSION'])\n\n","pos":24,"scrolled":true,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":61,"id":"1dc695","input":"\"\"\"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.svm import SVC as svc\nfrom sklearn import preprocessing\nfrom scipy import stats\n\n# target = heart_df['HeartDisease']\n# input_columns = heart_df.loc[:, heart_df.columns != \"HeartDisease\"]\n\n# # splitting the data\n# x_train, x_test, y_train, y_test = train_test_split(input_columns, target, test_size=0.2, random_state = 45)\n\nmdl = svc()\n# 'kernel': ('poly', 'linear', 'rbf', 'sigmoid')\n\nrand_list = {'C': list(range(1, 100)), 'gamma': list(range(1, 100)), 'kernel': ['poly', 'linear', 'rbf', 'sigmoid']}\nrand_search = RandomizedSearchCV(mdl, rand_list, n_iter = 100, cv = 5, verbose=100)\nrand_search.fit(x_train, y_train)\nrand_search.best_params_\"\"\"","kernel":"ds_env","output":{"0":{"name":"stdout","text":"Fitting 5 folds for each of 100 candidates, totalling 500 fits\n[CV 1/5; 1/100] START C=57, gamma=20, kernel=rbf................................\n[CV 1/5; 1/100] END .C=57, gamma=20, kernel=rbf;, score=0.531 total time=   0.0s\n[CV 2/5; 1/100] START C=57, gamma=20, kernel=rbf................................\n[CV 2/5; 1/100] END .C=57, gamma=20, kernel=rbf;, score=0.531 total time=   0.0s\n[CV 3/5; 1/100] START C=57, gamma=20, kernel=rbf................................\n[CV 3/5; 1/100] END .C=57, gamma=20, kernel=rbf;, score=0.531 total time=   0.0s\n[CV 4/5; 1/100] START C=57, gamma=20, kernel=rbf................................\n[CV 4/5; 1/100] END .C=57, gamma=20, kernel=rbf;, score=0.531 total time=   0.0s\n[CV 5/5; 1/100] START C=57, gamma=20, kernel=rbf................................\n[CV 5/5; 1/100] END .C=57, gamma=20, kernel=rbf;, score=0.534 total time=   0.0s\n[CV 1/5; 2/100] START C=62, gamma=72, kernel=poly...............................\n"}},"pos":45,"scrolled":true,"state":"done","type":"cell"}
{"cell_type":"code","id":"b8439d","input":"","pos":7,"type":"cell"}
{"cell_type":"code","id":"f42af9","input":"","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"1de317","input":"grid search cv","pos":13,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"3d6727","input":"Support Vector Classification Model uses two groups of data to categorize new data.  It does this by plotting the data onto a graph, and then using a hyperplane to split the data according to the group that the data belongs to. The hyper\\-parameters  that could be fine tuned are the Kernel, the C value, the Gamma value, and the Degree. I decided to use a RandomizedSearchCV to determine the best parameters.  When testing, for the best parameters, I ran into some problems. The first problem was how long it took too long to run the program. The program would run for about thirty minutes, and still not find a best fit for the graph. In response, I decided to decrease the amount of variables the RandomizedSearchCV needed to look for, and additionally, the range that each variable could search within. In the end, I decided that testing the C value, the Gamma value, and the Kernel would be most helpful in optimizing the model. According to my values the RandomizedSearchCV determine the best parameters was a Kernel of linear, a Gamma value of 96, and a C value of 9.\n\n","pos":50,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"594fc7","input":"The model is a logistic regression and it works by predicting whether someone has heart disease or if they do not. It classifies them into one of these two groups . We changed some parameters such as the weight, training and testing data. I think it performed well in predicting if someone has heart disease and classifying them into two groups. It does this by analyzing relationships between two independent variables. The coefficient of the linear regression model shows the relationship between the variables by multiplying it with a certain value and a higher/lower coefficient indicates which classification the data point would be predicted to be in.\n\n","pos":23,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"c0d2b5","input":"","pos":28,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"c6a4b2","input":"Adaboost is an example of ensemble learning, where several basic algorithms are combined to make one more optimized algorithm. For example, with a decision tree, instead of hoping 1 decision tree can correctly guess a result, ensemble learning will take multiple decision trees and combine them together to create a stronger algorithm.\n\nAdaboost is also a sequential learning version of ensemble learning, where every model is made in order, and the newest model learns from the previous model's failures. It does this by \"boosting\", or reducing the error through methods such as increasing the weights of data that was mislabelled.\n\nHowever, Adaboost is extremely sensitive to noisy data and outliers, which is why it is highly recommended to remove outliers from data with Adaboost.\n\nHere, Adaboost was used on our heart disease dataset, with a mean squared error of 0.12 on our testing data, and an overall accuracy score of 0.880 and a precision score of 0.880. There were 13 false positives and 9 false negatives. This is a decent score for a machine learning model, but when it comes to diagnosis it probably isn't the best and there is certainly a more ideal model. \n","pos":15,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"d38595","input":"**NOTE:** PLEASE save the scores to the score dictionary \n\nModels to try out:\n\n- Decision Tree \\(Emma\\)\n- Random Forest \\(Emma\\)\n- Adaboost\n- SVC\n- \n- NEURAL NETWORKS  Thomas\n- KNN\n- Logistic Regression \\(kenju\\)\n- Naive bayes\n- \n- \n- Naive Bayes \\(Rhone\\)\n- Adaboost \\(Jeffrey\\)\n- Support vector classifier \\(halli\\)\n- \n- Stochastic Gradient Descent Classifier \\(SGDClassifier\\)\n  - https://scikit\\-learn.org/stable/modules/generated/sklearn.linear\\_model.SGDClassifier.html\n- Support vector classifier \n- KNN \\(Rhone\\)\n- \n- \n\n<u>**AlSO TRY OPTIMIZING THEM**</u>\n\nGrid Search CV\n\nEvaluation metrics:\n\n- F1 score\n- Accuracy\n- Recall\n- Precision\n- Confusion matrix\n\n","pos":6,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"e37112","input":"Gaussian Naive Bayes is for binary classification and assumes that all continuous data has normal distribution, and that all features are independent and equal from each other. The Naive Bayes finds the probability of the predicted outcome\\(y variable\\) given the x values and then classifies the point based on whether one of the classifications had a higher probability than the other classification. In order to optimize the function, I stratified the data, which raised the accuracy score from 0.83 to 0.885 while also scaling the data though that did not cause a change in the accuracy score or any of the other measures. The results ending up giving a accuracy score of 0.885, a precision score of 0.878, a recall score of 0.921, and an f1 score of 0.899. This resulted in 8 false negatives and 13 false positives out of 184 points in what I believe can be described as an accurate model.\n\n","pos":32,"state":"done","type":"cell"}
{"cell_type":"markdown","id":"f8ef79","input":"The KNN nearest neighbors function looks for the k nearest neighbors of a point and their classification and then classifies the point based on the state of the nearest neighbors. Using GridSearchCV, I tried to optimize the value of the k nearest neighbors in a range from 1 to 400 and it was found that 23 was the most optimal number. It ended up giving a success rate of roughly 72.8 percent with 25 false positives and 25 false negative, 57 correct positives, and 77 correct negatives, a recall rate of 75.5 percent a precision rate of 75.5 percent, and an f1\\_score of 75.5 percent. These numbers would be described as a moderately \\-to good accurate model.\n\n","pos":18,"state":"done","type":"cell"}
{"id":"ee008d","input":"","pos":35.5,"type":"cell"}
{"id":0,"time":1659626227649,"type":"user"}
{"last_load":1659621497944,"type":"file"}