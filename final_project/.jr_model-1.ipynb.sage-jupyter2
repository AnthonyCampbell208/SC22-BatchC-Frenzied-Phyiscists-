{"backend_state":"init","connection_file":"/projects/d5a3aa23-997d-4ffc-977a-1dc20c583e62/.local/share/jupyter/runtime/kernel-32441813-b1b6-456d-933c-c025790e228b.json","kernel":"ds_env","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"12eae3","input":"","pos":10,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"231197","input":"y_hat = my_SVC_model.predict(x_test)\ntotal_squared_error = (np.sum((y_test - y_hat)**2)) #get the sum of all the errors (error = what we want (y_test) - what we predicted (y_hat))\nmean_squared_error = total_squared_error/len(y_test) #divide this by how many rows/observations we have \nprint(mean_squared_error)","pos":48,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"2c6283","input":"","pos":5,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"381848","input":"x_train","pos":43,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"384c47","input":"svc_pred","pos":45,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"3e5325","input":"","pos":12,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"4a23bf","input":"y_hat","pos":39,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"4eced4","input":"#MSE\n#total_squared_error = (np.sum((y_test - y_hat)**2)) #get the sum of all the errors (error = what we want (y_test) - what we predicted (y_hat))\n#mean_squared_error = total_squared_error/len(y_test) #divide this by how many rows/observations we have \n#print(mean_squared_error)","pos":41,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"51061e","input":"","pos":0,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"57bce8","input":"#Confusion Matrix\nsns.heatmap(confusion_matrix(y_test, y_hat), annot=True, fmt='g')\nacc = accuracy_score(y_test, y_hat)\nfone_score = f1_score(y_test, y_hat)\nrec = recall_score(y_test, y_hat)\nprec = precision_score(y_test, y_hat)\n\n#accuracy, f1 score, recall, precision\nprint(\"Accuracy: \" , acc , \"f1 Score: \" , fone_score, \"Recall: \" , rec, \"Precision: \" , prec)","pos":49,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"5f1d42","input":"","pos":35,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"6e050d","input":"","pos":8,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"713310","input":"heart_dfs = heart_df.copy()\ntarget = heart_dfs[\"HeartDisease\"]\ninput_columns = heart_dfs.loc[:, heart_dfs.columns != \"HeartDisease\"]\n#sc = StandardScaler()\n#x_train = sc.fit_transform(x_train)\n#x_test = sc.transform(x_test)\n\n\n\n\nx_train, x_test, y_train, y_test = train_test_split(input_columns, target, test_size=0.2,stratify =target, random_state=42)\nKNN_OLD_MODEL = KNN()\nKNN_model = KNN(n_neighbors = 23)\nKNN_model.fit(x_train,y_train)\n\ny_hat = KNN_model.predict(x_test)\n\ntotal_squared_error = (np.sum((y_test - y_hat)**2)) #get the sum of all the errors (error = what we want (y_test) - what we predicted (y_hat))\nmean_squared_error = total_squared_error/len(y_test) #divide this by how many rows/observations we have \nprint(mean_squared_error)\n\n\ngop = sns.heatmap(confusion_matrix(y_test, y_hat), annot=True, fmt='g')\nprint(gop)\nacc = accuracy_score(y_test, y_hat)\nprec = precision_score(y_test, y_hat)\nrecall = recall_score(y_test, y_hat)\nf1 = f1_score(y_test, y_hat)\nscores['KNN_ADJUSTED'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}\nprint(scores['KNN_ADJUSTED'])\ngop.write_html(\"confusion_matrices/gop.png\")\n\n\n","pos":18,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"7d3ebe","input":"#from sklearn.model_selection import GridSearchCV\n#from sklearn.svm import SVC as svc\n#from sklearn import preprocessing\n#from scipy import stats\n\n#mdl = svc()\n#param_grid = {\"C\": [.1, 1, 10, 100, 1000], 'gamma': [.1, 1, 10, 100, 1000], 'kernel': ['poly', 'linear', 'rbf', 'sigmoid']}\n#grid = GridSearchCV(mdl, param_grid, refit = True, verbose = 7)\n#grid.fit(x_train, y_train)\n\n\n#grid_search = GridSearchCV(mdl, param_grid = grid_list, cv = 5)\n#grid_search.fit(x_train, y_train)\n#print(grid.best_params_)\n#print(\"hi\")","pos":50,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"8e1b9f","input":"","pos":52,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"a09488","input":"np.array(y_test)","pos":40,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"b928b6","input":"# define x as all columns but heart disease\n\nX = heart_df.loc[:, heart_df.columns != \"HeartDisease\"]\nX = (X - np.min(X)) / (np.max(X) - np.min(X))\n#define y as heart_disease\ny = heart_df['HeartDisease']\n#split data set 80 percent train: 20 percent test\n\n\nn_iter = 1000\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y)\n#scales down x_values\nst_x= StandardScaler()\nx_train= st_x.fit_transform(X_train)\nx_test= st_x.transform(X_test)\nthresholder = VarianceThreshold(threshold=.5)\n\nx_train = thresholder.fit_transform(x_train)\nx_test = thresholder.fit_transform(x_test)\n#create object model\ngnb = GaussianNB(priors=None, var_smoothing=1e-06)\n#fit object model\ngnb.fit(x_train, y_train)\nGaussianNB(priors=None, var_smoothing= 1)\ny_pred = gnb.predict(x_test)\n#print(\"Naive Bayes score: \",gnb.score(X_test, y_test))#\n#print(\"Number of mislabeled points out of a total %d points : %d\"\n\n\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nljo= sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\nprint(ljo)\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\nscores['NAIVEBAYES'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}\nprint(scores['NAIVEBAYES'])\nprint(\"Naive Bayes score: \",gnb.score(x_test, y_test))\nljo.write_html(\"confusion_matrices/ljo.png\")","pos":32,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"c63739","input":"#Confusion Matrix\n#sns.heatmap(confusion_matrix(y_test, y_hat), annot=True, fmt='g')\n#acc = accuracy_score(y_test, y_hat)\n#fone_score = f1_score(y_test, y_hat)\n#rec = recall_score(y_test, y_hat)\n#prec = precision_score(y_test, y_hat)\n\n#accuracy, f1 score, recall, precision\n#print(\"Accuracy: \" , acc , \"f1 Score: \" , fone_score, \"Recall: \" , rec, \"Precision: \" , prec)","pos":42,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"d9ea86","input":"(89)/(89+12)\n(65)/(65+18)","pos":34,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"dab4d4","input":"from sklearn.svm import SVC\n\n#changing the kernel\nmy_SVC_model = SVC(kernel = 'linear', 'c' = 9, 'gamma' = 96)\n\n#fitting the model\nmy_SVC_model.fit(x_train, y_train)","pos":47,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"fbdc0e","input":"\"\"\"heart_dfs = heart_df.copy()\ntarget = heart_dfs[\"HeartDisease\"]\ninput_columns = heart_dfs.loc[:, heart_dfs.columns != \"HeartDisease\"]\ny = heart_df[\"HeartDisease\"]\nparam_grid = {'var_smoothing':range(0,200)}\nx = GaussianNB()\ngrid_x = GridSearchCV(x, param_grid)\nx_train, x_test, y_train, y_test = train_test_split(input_columns, target, test_size=0.2,stratify =y, random_state=42)\n\ngrid_x.fit(x_train, y_train)\nprint(grid_x.best_params_)\"\"\"","pos":31,"type":"cell"}
{"cell_type":"code","exec_count":10,"id":"f98365","input":"heart_df = pd.read_csv(\"data/heart.csv\")\nheart_df\n","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>ChestPainType</th>\n      <th>RestingBP</th>\n      <th>Cholesterol</th>\n      <th>FastingBS</th>\n      <th>RestingECG</th>\n      <th>MaxHR</th>\n      <th>ExerciseAngina</th>\n      <th>Oldpeak</th>\n      <th>ST_Slope</th>\n      <th>HeartDisease</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>40</td>\n      <td>M</td>\n      <td>ATA</td>\n      <td>140</td>\n      <td>289</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>172</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Up</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>49</td>\n      <td>F</td>\n      <td>NAP</td>\n      <td>160</td>\n      <td>180</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>156</td>\n      <td>N</td>\n      <td>1.0</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>37</td>\n      <td>M</td>\n      <td>ATA</td>\n      <td>130</td>\n      <td>283</td>\n      <td>0</td>\n      <td>ST</td>\n      <td>98</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Up</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>48</td>\n      <td>F</td>\n      <td>ASY</td>\n      <td>138</td>\n      <td>214</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>108</td>\n      <td>Y</td>\n      <td>1.5</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>54</td>\n      <td>M</td>\n      <td>NAP</td>\n      <td>150</td>\n      <td>195</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>122</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Up</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>913</th>\n      <td>45</td>\n      <td>M</td>\n      <td>TA</td>\n      <td>110</td>\n      <td>264</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>132</td>\n      <td>N</td>\n      <td>1.2</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>914</th>\n      <td>68</td>\n      <td>M</td>\n      <td>ASY</td>\n      <td>144</td>\n      <td>193</td>\n      <td>1</td>\n      <td>Normal</td>\n      <td>141</td>\n      <td>N</td>\n      <td>3.4</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>915</th>\n      <td>57</td>\n      <td>M</td>\n      <td>ASY</td>\n      <td>130</td>\n      <td>131</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>115</td>\n      <td>Y</td>\n      <td>1.2</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>916</th>\n      <td>57</td>\n      <td>F</td>\n      <td>ATA</td>\n      <td>130</td>\n      <td>236</td>\n      <td>0</td>\n      <td>LVH</td>\n      <td>174</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Flat</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>917</th>\n      <td>38</td>\n      <td>M</td>\n      <td>NAP</td>\n      <td>138</td>\n      <td>175</td>\n      <td>0</td>\n      <td>Normal</td>\n      <td>173</td>\n      <td>N</td>\n      <td>0.0</td>\n      <td>Up</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>918 rows × 12 columns</p>\n</div>","text/plain":"     Age Sex ChestPainType  RestingBP  Cholesterol  FastingBS RestingECG  \\\n0     40   M           ATA        140          289          0     Normal   \n1     49   F           NAP        160          180          0     Normal   \n2     37   M           ATA        130          283          0         ST   \n3     48   F           ASY        138          214          0     Normal   \n4     54   M           NAP        150          195          0     Normal   \n..   ...  ..           ...        ...          ...        ...        ...   \n913   45   M            TA        110          264          0     Normal   \n914   68   M           ASY        144          193          1     Normal   \n915   57   M           ASY        130          131          0     Normal   \n916   57   F           ATA        130          236          0        LVH   \n917   38   M           NAP        138          175          0     Normal   \n\n     MaxHR ExerciseAngina  Oldpeak ST_Slope  HeartDisease  \n0      172              N      0.0       Up             0  \n1      156              N      1.0     Flat             1  \n2       98              N      0.0       Up             0  \n3      108              Y      1.5     Flat             1  \n4      122              N      0.0       Up             0  \n..     ...            ...      ...      ...           ...  \n913    132              N      1.2     Flat             1  \n914    141              N      3.4     Flat             1  \n915    115              Y      1.2     Flat             1  \n916    174              N      0.0     Flat             1  \n917    173              N      0.0       Up             0  \n\n[918 rows x 12 columns]"},"exec_count":10,"output_type":"execute_result"}},"pos":3,"type":"cell"}
{"cell_type":"code","exec_count":11,"id":"e74ca9","input":"#heart_df['female'] = heart_df['female'].map({'F': 1, 'M': 0})\nheart_df['Sex'].replace('F',0 ,inplace=True)\nheart_df['Sex'].replace('M', 1,inplace=True)\n#Female is 0\n#Male is 1\n\n#ATA is 0\n#NAP is 1\n#ASY is 2\n#TA is 3\nheart_df['ChestPainType'].replace('ATA',0 ,inplace=True)\nheart_df['ChestPainType'].replace('NAP',1 ,inplace=True)\nheart_df['ChestPainType'].replace('ASY',2 ,inplace=True)\nheart_df['ChestPainType'].replace('TA',3 ,inplace=True)\n\n#Normal is 0\n#St is 1\n#LVH is 2\nheart_df['RestingECG'].replace('Normal',0, inplace=True)\nheart_df['RestingECG'].replace('ST',1, inplace=True)\nheart_df['RestingECG'].replace('LVH',2, inplace=True)\n\n#No is 0\n#Yes is 1\nheart_df['ExerciseAngina'].replace('N',0 ,inplace=True)\nheart_df['ExerciseAngina'].replace('Y',1 ,inplace=True)\n\n#ST_Slope\n#Up is 0\n#Flat is 1\n#Down is 2\nheart_df['ST_Slope'].replace('Up', 0, inplace = True)\nheart_df['ST_Slope'].replace('Flat', 1, inplace = True)\nheart_df['ST_Slope'].replace('Down', 2, inplace = True)\n\nheart_df.head() #worky :)","output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>ChestPainType</th>\n      <th>RestingBP</th>\n      <th>Cholesterol</th>\n      <th>FastingBS</th>\n      <th>RestingECG</th>\n      <th>MaxHR</th>\n      <th>ExerciseAngina</th>\n      <th>Oldpeak</th>\n      <th>ST_Slope</th>\n      <th>HeartDisease</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>40</td>\n      <td>1</td>\n      <td>0</td>\n      <td>140</td>\n      <td>289</td>\n      <td>0</td>\n      <td>0</td>\n      <td>172</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>49</td>\n      <td>0</td>\n      <td>1</td>\n      <td>160</td>\n      <td>180</td>\n      <td>0</td>\n      <td>0</td>\n      <td>156</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>37</td>\n      <td>1</td>\n      <td>0</td>\n      <td>130</td>\n      <td>283</td>\n      <td>0</td>\n      <td>1</td>\n      <td>98</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>48</td>\n      <td>0</td>\n      <td>2</td>\n      <td>138</td>\n      <td>214</td>\n      <td>0</td>\n      <td>0</td>\n      <td>108</td>\n      <td>1</td>\n      <td>1.5</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>54</td>\n      <td>1</td>\n      <td>1</td>\n      <td>150</td>\n      <td>195</td>\n      <td>0</td>\n      <td>0</td>\n      <td>122</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   Age  Sex  ChestPainType  RestingBP  Cholesterol  FastingBS  RestingECG  \\\n0   40    1              0        140          289          0           0   \n1   49    0              1        160          180          0           0   \n2   37    1              0        130          283          0           1   \n3   48    0              2        138          214          0           0   \n4   54    1              1        150          195          0           0   \n\n   MaxHR  ExerciseAngina  Oldpeak  ST_Slope  HeartDisease  \n0    172               0      0.0         0             0  \n1    156               0      1.0         1             1  \n2     98               0      0.0         0             0  \n3    108               1      1.5         1             1  \n4    122               0      0.0         0             0  "},"exec_count":11,"output_type":"execute_result"}},"pos":4,"type":"cell"}
{"cell_type":"code","exec_count":12,"id":"226f4e","input":"heart_df.dropna(inplace=True)\nheart_df.shape","output":{"0":{"data":{"text/plain":"(918, 12)"},"exec_count":12,"output_type":"execute_result"}},"pos":6,"type":"cell"}
{"cell_type":"code","exec_count":13,"id":"76b237","input":"scores = {}\nscores['metrics'] = {'accuracy': 'accuracy', 'precision':'precision', 'recall':'recall', 'f1_score':'f1_score'}","pos":9,"type":"cell"}
{"cell_type":"code","exec_count":14,"id":"01b795","input":"target = heart_df[\"HeartDisease\"]\ninput_columns = heart_df.loc[:, heart_df.columns != \"HeartDisease\"]\nx_train, x_test, y_train, y_test = train_test_split(input_columns, target, test_size=0.3)\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","pos":11,"type":"cell"}
{"cell_type":"code","exec_count":15,"id":"56408e","input":"x_test.shape","output":{"0":{"data":{"text/plain":"(276, 11)"},"exec_count":15,"output_type":"execute_result"}},"pos":13,"type":"cell"}
{"cell_type":"code","exec_count":16,"id":"475244","input":"#ADABOOST\n# parameters = {'n_estimators':[50, 100, 125, 150], 'learning_rate': [0.01, 0.1, 0.3, 0.5]}\n# scoring = ['precision', 'accuracy', 'recall', 'f1']\n\n\n# for score in scoring:\n#     abc = AdaBoostClassifier()\n#     grid_abc = GridSearchCV(abc, parameters, scoring=score)\n#     grid_abc.fit(x_train, y_train)\n\n\n#     print(\"best params for %s: \" % score)\n#     print(grid_abc.best_params_)\n\n#     means = grid_abc.cv_results_[\"mean_test_score\"]\n\n#     print(\"mean scores\")\n#     for mean, params in zip(means, grid_abc.cv_results_[\"params\"]):\n#         print(\"%0.5f for %r\" % (mean, params))\n\nabc = AdaBoostClassifier(n_estimators=100, learning_rate=0.1)\nabc.fit(x_train, y_train)\n\ny_predictions = abc.predict(x_test)\n\ntotal_squared_error = (np.sum((y_test - y_predictions)**2)) #get the sum of all the errors (error = what we want (y_test) - what we predicted (y_hat))\nmean_squared_error = total_squared_error/len(y_test) #divide this by how many rows/observations we have \n#print(mean_squared_error)\nsns.heatmap(confusion_matrix(y_test, y_predictions), annot=True, fmt='g')\nacc = accuracy_score(y_test, y_predictions)\nprec = precision_score(y_test, y_predictions, average='micro')\nrecall = recall_score(y_test, y_predictions, average='micro')\nf1 = f1_score(y_test, y_predictions, average='micro')\nscores['Adaboost'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}\nprint(scores['Adaboost'])\n\n","output":{"0":{"name":"stdout","output_type":"stream","text":"{'accuracy': 0.8623188405797102, 'precision': 0.8623188405797102, 'recall': 0.8623188405797102, 'f1_score': 0.8623188405797102}\n"},"1":{"data":{"image/png":"c8b003cfb30b7ba05c8a660ead70bf6eb2863e10","text/plain":"<Figure size 432x288 with 2 Axes>"},"exec_count":16,"metadata":{"needs_background":"light"},"output_type":"execute_result"}},"pos":15,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":17,"id":"ce99fd","input":"heart_dfs = heart_df.copy()\ntarget = heart_dfs[\"HeartDisease\"]\ninput_columns = heart_dfs.loc[:, heart_dfs.columns != \"HeartDisease\"]\ny = heart_df[\"HeartDisease\"]\nparam_grid = {'n_neighbors': range(1, 400)}\n\nx = KNN()\ngrid_x = GridSearchCV(x, param_grid)\nx_train, x_test, y_train, y_test = train_test_split(input_columns, target, test_size=0.2,stratify =y, random_state=42)\n\ngrid_x.fit(x_train, y_train)\nprint(grid_x.best_params_)","output":{"0":{"name":"stdout","output_type":"stream","text":"{'n_neighbors': 23}\n"}},"pos":17,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":19,"id":"cf0468","input":"\"\"\"#y_hat = KNN_model.predict(x_test)\nmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#00AAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])\n\nclf = neighbors.KNeighborsClassifier(n_neighbors = 23, weights='distance')\nclf.fit(x_train, y_train)\n\nx_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\ny_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\"\"\"","output":{"0":{"data":{"text/plain":"\"#y_hat = KNN_model.predict(x_test)\\nmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#00AAFF'])\\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#00AAFF'])\\n\\nclf = neighbors.KNeighborsClassifier(n_neighbors = 23, weights='distance')\\nclf.fit(x_train, y_train)\\n\\nx_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\\ny_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\\n                     np.arange(y_min, y_max, h))\\n\\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\""},"exec_count":19,"output_type":"execute_result"}},"pos":20,"type":"cell"}
{"cell_type":"code","exec_count":20,"id":"ee6900","input":"#print(y_hat)\nprint(np.array(y_test))\n\n\"\"\"total_squared_error = (np.sum((y_test - y_hat)**2)) #get the sum of all the errors (error = what we want (y_test) - what we predicted (y_hat))\nmean_squared_error = total_squared_error/len(y_test) #divide this by how many rows/observations we have \nprint(mean_squared_error)\"\"\"\n\"\"\"sns.heatmap(confusion_matrix(y_test, y_hat), annot=True, fmt='g')\nacc = accuracy_score(y_test, y_hat)\nprec = precision_score(y_test, y_hat, average='micro')\nrecall = recall_score(y_test, y_hat, average='micro')\nf1 = f1_score(y_test, y_hat, average='micro')\nscores['KNN'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}\nprint(scores['KNN'])\"\"\"","output":{"0":{"name":"stdout","output_type":"stream","text":"[1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1\n 1 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1\n 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1\n 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0\n 1 1 1 0 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1]\n"},"1":{"data":{"text/plain":"\"sns.heatmap(confusion_matrix(y_test, y_hat), annot=True, fmt='g')\\nacc = accuracy_score(y_test, y_hat)\\nprec = precision_score(y_test, y_hat, average='micro')\\nrecall = recall_score(y_test, y_hat, average='micro')\\nf1 = f1_score(y_test, y_hat, average='micro')\\nscores['KNN'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}\\nprint(scores['KNN'])\""},"exec_count":20,"output_type":"execute_result"}},"pos":21,"type":"cell"}
{"cell_type":"code","exec_count":21,"id":"6e44db","input":"mlp = MLPClassifier(solver='lbfgs',hidden_layer_sizes=(1000, 35), alpha=0.05, max_iter=200, random_state = 1, activation='relu', learning_rate='adaptive')\nmlp.fit(x_train, y_train)\ny_hat = mlp.predict(x_test)\nprint(y_hat)\nprint(np.array(y_test))\ntotal_squared_error = (np.sum((y_test - y_hat)**2)) #get the sum of all the errors (error = what we want (y_test) - what we predicted (y_hat))\nmean_squared_error = total_squared_error/len(y_test) #divide this by how many rows/observations we have \nprint(mean_squared_error)\nsns.heatmap(confusion_matrix(y_test, y_hat), annot=True, fmt='g')\nacc = accuracy_score(y_test, y_hat)\nprec = precision_score(y_test, y_hat, average='micro')\nrecall = recall_score(y_test, y_hat, average='micro')\nf1 = f1_score(y_test, y_hat, average='micro')\nscores['mlp'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}\nprint(scores['mlp'])","output":{"0":{"name":"stderr","output_type":"stream","text":"/projects/d5a3aa23-997d-4ffc-977a-1dc20c583e62/miniconda3/envs/ds_env/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\n  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"},"1":{"name":"stdout","output_type":"stream","text":"[1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 0 0 1 0 0 1 1 1\n 1 0 1 1 0 1 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 0 0 0 1 1 1\n 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1\n 1 0 1 1 1 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1\n 1 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 1 1 1]\n[1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1\n 1 0 1 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1\n 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 1\n 0 0 1 1 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0\n 1 1 1 0 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 1 1]\n0.16304347826086957\n{'accuracy': 0.8369565217391305, 'precision': 0.8369565217391305, 'recall': 0.8369565217391305, 'f1_score': 0.8369565217391305}\n"},"2":{"data":{"image/png":"cae88457aa26460aecf0200d826df5436b19315f","text/plain":"<Figure size 432x288 with 2 Axes>"},"exec_count":21,"metadata":{"needs_background":"light"},"output_type":"execute_result"}},"pos":22,"type":"cell"}
{"cell_type":"code","exec_count":22,"id":"7cc89f","input":"# Please save scores like the example below\nscores['knn'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}","pos":23,"type":"cell"}
{"cell_type":"code","exec_count":23,"id":"4df1f9","input":"#x is everything but heart disease\nx = heart_df.loc[:, heart_df.columns != \"HeartDisease\"]\n#y is heart disease\ny = heart_df['HeartDisease']\n# splits dataset; 80 percent train: 20 percent test\nx_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.2, random_state=0)\n#scales down the x values\nst_x= StandardScaler()\nx_train= st_x.fit_transform(x_train)\nx_test= st_x.transform(x_test)\nlr_clf = LogisticRegression(random_state=42)\n#Fits dataset\nlr_clf.fit(x_train, y_train)\n#gets predicted values\ny_pred= lr_clf.predict(x_test)\n#compares predictions to actual values\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\n\n\"\"\"total_squared_error = (np.sum((y_test - y_pred)**2))\nmean_squared_error = total_squared_error/len(y_test)\nprint(mean_squared_error)\"\"\"\n\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\nscores['LOGISTICREGRESSION'] = {'accuracy': acc, 'precision':prec, 'recall':recall, 'f1_score':f1}\nprint(scores['LOGISTICREGRESSION'])\n\n","output":{"0":{"name":"stdout","output_type":"stream","text":"{'accuracy': 0.8478260869565217, 'precision': 0.8495575221238938, 'recall': 0.897196261682243, 'f1_score': 0.8727272727272727}\n"},"1":{"data":{"image/png":"0dc2221b6f39dd0fd704be0c36c39e9b414dee61","text/plain":"<Figure size 432x288 with 3 Axes>"},"exec_count":23,"metadata":{"needs_background":"light"},"output_type":"execute_result"}},"pos":25,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":24,"id":"c0e2d8","input":"heart_df.columns","output":{"0":{"data":{"text/plain":"Index(['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS',\n       'RestingECG', 'MaxHR', 'ExerciseAngina', 'Oldpeak', 'ST_Slope',\n       'HeartDisease'],\n      dtype='object')"},"exec_count":24,"output_type":"execute_result"}},"pos":26,"type":"cell"}
{"cell_type":"code","exec_count":25,"id":"49e4f7","input":"lr_clf.coef_","output":{"0":{"data":{"text/plain":"array([[-0.05035417,  0.52130825,  0.70399177,  0.08542451, -0.25692624,\n         0.46711163,  0.05839025, -0.28834911,  0.56634088,  0.36548402,\n         1.0337979 ]])"},"exec_count":25,"output_type":"execute_result"}},"pos":27,"type":"cell"}
{"cell_type":"code","exec_count":26,"id":"7d6daa","input":"lr_clf.intercept_","output":{"0":{"data":{"text/plain":"array([0.33897953])"},"exec_count":26,"output_type":"execute_result"}},"pos":28,"type":"cell"}
{"cell_type":"code","exec_count":27,"id":"cb5195","input":"y_pred_proba = classifier.predict_proba(x_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"heart dataframe, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()\n\"\"\"fpr = dict()\ntpr = dict()\nroc_auc = dict()\n\ny = label_binarize(y, classes=[0, 1, 2,])\nn_classes = y.shape[1]\n\n\nfor i in y_test:\n    i = int(i)\ny_test = list(y_test)\nfor i in y_pred:\n    i = int(i)\ny_pred = list(y_pred)\nprint(type(y_test))\nfor i in y_pred:\n    for x in y_test:\n        if i == -1 <= x:\n            print(i)\n\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[0:i], y_pred[0:i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\nplt.figure()\nlw = 2\nplt.plot(\n    fpr[2],\n    tpr[2],\n    color=\"darkorange\",\n    lw=lw,\n    label=\"ROC curve (area = %0.2f)\" % roc_auc[2],\n)\nplt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver operating characteristic example\")\nplt.legend(loc=\"lower right\")\nplt.show()\n\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n#This plots the tradeoff of positive rate to false positive rate(roc curve)\"\"\"\n\"\"\"fitted_y = np.array(y_pred)\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(heart_df.loc[:, heart_df.columns != \"HeartDisease\"],heart_df['HeartDisease'],c='red', marker='o', alpha=0.5)\nax.plot_surface(x_test,y_test,y_pred.reshape(x_test.shape), color='b', alpha=0.3)\nax.set_xlabel('Price')\nax.set_ylabel('AdSpends')\nax.set_zlabel('Sales')\nplt.show()\"\"\"","output":{"0":{"ename":"NameError","evalue":"name 'classifier' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred_proba \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[38;5;241m.\u001b[39mpredict_proba(x_test)[::,\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      2\u001b[0m fpr, tpr, _ \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mroc_curve(y_test, y_pred_proba)\n\u001b[1;32m      3\u001b[0m auc \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mroc_auc_score(y_test, y_pred_proba)\n","\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"]}},"pos":30,"type":"cell"}
{"cell_type":"code","exec_count":29,"id":"a2b84a","input":"# SVC Model\n\n#configuring the data\ntarget = heart_df['HeartDisease']\ninput_columns = heart_df.loc[:, heart_df.columns != \"HeartDisease\"]\n\n#splitting the data\nx_train, x_test, y_train, y_test = train_test_split(input_columns, target, test_size=0.2)\n\nx_train.shape","output":{"0":{"data":{"text/plain":"(734, 11)"},"exec_count":29,"output_type":"execute_result"}},"pos":36,"type":"cell"}
{"cell_type":"code","exec_count":30,"id":"ebf5c6","input":"from sklearn.svm import SVC\n\n#changing the kernel\nmy_SVC_model = SVC(kernel = 'rbf')\n\n#fitting the model\nmy_SVC_model.fit(x_train, y_train)","output":{"0":{"data":{"text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>","text/plain":"SVC()"},"exec_count":30,"output_type":"execute_result"}},"pos":37,"type":"cell"}
{"cell_type":"code","exec_count":31,"id":"d71b79","input":"#predicting results\ny_hat = my_SVC_model.predict(x_test)","pos":38,"type":"cell"}
{"cell_type":"code","exec_count":32,"id":"dbc6f0","input":"svc_clf = svc()\nsvc_clf.fit(x_train, y_train)\nsvc_pred = svc_clf.predict(x_test)\nprec = precision_score(y_test, svc_pred)\nprint(prec)","output":{"0":{"ename":"NameError","evalue":"name 'svc' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m svc_clf \u001b[38;5;241m=\u001b[39m \u001b[43msvc\u001b[49m()\n\u001b[1;32m      2\u001b[0m svc_clf\u001b[38;5;241m.\u001b[39mfit(x_train, y_train)\n\u001b[1;32m      3\u001b[0m svc_pred \u001b[38;5;241m=\u001b[39m svc_clf\u001b[38;5;241m.\u001b[39mpredict(x_test)\n","\u001b[0;31mNameError\u001b[0m: name 'svc' is not defined"]}},"pos":44,"type":"cell"}
{"cell_type":"code","exec_count":33,"id":"14226f","input":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.svm import SVC as svc\nfrom sklearn import preprocessing\nfrom scipy import stats\n\n# target = heart_df['HeartDisease']\n# input_columns = heart_df.loc[:, heart_df.columns != \"HeartDisease\"]\n\n# # splitting the data\n# x_train, x_test, y_train, y_test = train_test_split(input_columns, target, test_size=0.2, random_state = 45)\n\nmdl = svc()\n# 'kernel': ('poly', 'linear', 'rbf', 'sigmoid')\n\nrand_list = {'C': list(range(1, 100)), 'gamma': list(range(1, 100)), 'kernel': ['poly', 'linear', 'rbf', 'sigmoid']}\nrand_search = RandomizedSearchCV(mdl, rand_list, n_iter = 100, cv = 5, verbose=100)\nrand_search.fit(x_train, y_train)\nrand_search.best_params_","output":{"0":{"name":"stdout","output_type":"stream","text":"Fitting 5 folds for each of 100 candidates, totalling 500 fits\n[CV 1/5; 1/100] START C=30, gamma=34, kernel=rbf................................\n[CV 1/5; 1/100] END .C=30, gamma=34, kernel=rbf;, score=0.571 total time=   0.1s\n[CV 2/5; 1/100] START C=30, gamma=34, kernel=rbf................................\n[CV 2/5; 1/100] END .C=30, gamma=34, kernel=rbf;, score=0.565 total time=   0.0s\n[CV 3/5; 1/100] START C=30, gamma=34, kernel=rbf................................\n[CV 3/5; 1/100] END .C=30, gamma=34, kernel=rbf;, score=0.565 total time=   0.0s\n[CV 4/5; 1/100] START C=30, gamma=34, kernel=rbf................................\n[CV 4/5; 1/100] END .C=30, gamma=34, kernel=rbf;, score=0.565 total time=   0.0s\n[CV 5/5; 1/100] START C=30, gamma=34, kernel=rbf................................\n"},"1":{"name":"stdout","output_type":"stream","text":"[CV 5/5; 1/100] END .C=30, gamma=34, kernel=rbf;, score=0.568 total time=   0.0s\n[CV 1/5; 2/100] START C=43, gamma=68, kernel=rbf................................\n[CV 1/5; 2/100] END .C=43, gamma=68, kernel=rbf;, score=0.571 total time=   0.0s\n[CV 2/5; 2/100] START C=43, gamma=68, kernel=rbf................................\n[CV 2/5; 2/100] END .C=43, gamma=68, kernel=rbf;, score=0.565 total time=   0.0s\n[CV 3/5; 2/100] START C=43, gamma=68, kernel=rbf................................\n[CV 3/5; 2/100] END .C=43, gamma=68, kernel=rbf;, score=0.565 total time=   0.0s\n[CV 4/5; 2/100] START C=43, gamma=68, kernel=rbf................................\n[CV 4/5; 2/100] END .C=43, gamma=68, kernel=rbf;, score=0.565 total time=   0.0s\n[CV 5/5; 2/100] START C=43, gamma=68, kernel=rbf................................\n[CV 5/5; 2/100] END .C=43, gamma=68, kernel=rbf;, score=0.568 total time=   0.0s\n[CV 1/5; 3/100] START C=18, gamma=97, kernel=poly...............................\n"}},"pos":46,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":8,"id":"0680ac","input":"import pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport numpy\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns\nimport sklearn\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import neighbors, datasets\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import StandardScaler","pos":1,"type":"cell"}
{"cell_type":"code","exec_count":9,"id":"333717","input":"np.arange(60,100, 5)","output":{"0":{"data":{"text/plain":"array([60, 65, 70, 75, 80, 85, 90, 95])"},"exec_count":9,"output_type":"execute_result"}},"pos":2,"type":"cell"}
{"cell_type":"markdown","id":"05a0d4","input":"**NOTE:** PLEASE save the scores to the score dictionary \n\nModels to try out:\n\n- Decision Tree \\(Emma\\)\n- Random Forest \\(Emma\\)\n- Adaboost\n- SVC\n- \n- NEURAL NETWORKS  Thomas\n- KNN\n- Logistic Regression \\(kenju\\)\n- Naive bayes\n- \n- \n- Naive Bayes \\(Rhone\\)\n- Adaboost \\(Jeffrey\\)\n- Support vector classifier \\(halli\\)\n- \n- Stochastic Gradient Descent Classifier \\(SGDClassifier\\)\n  - https://scikit\\-learn.org/stable/modules/generated/sklearn.linear\\_model.SGDClassifier.html\n- Support vector classifier \n- KNN \\(Rhone\\)\n- \n- \n\n<u>**AlSO TRY OPTIMIZING THEM**</u>\n\nGrid Search CV\n\nEvaluation metrics:\n\n- F1 score\n- Accuracy\n- Recall\n- Precision\n- Confusion matrix\n\n","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"165f1e","input":"Adaboost is an example of ensemble learning, where several basic algorithms are combined to make one more optimized algorithm. For example, with a decision tree, instead of hoping 1 decision tree can correctly guess a result, ensemble learning will take multiple decision trees and combine them together to create a stronger algorithm.\n\nAdaboost is also a sequential learning version of ensemble learning, where every model is made in order, and the newest model learns from the previous model's failures. It does this by \"boosting\", or reducing the error through methods such as increasing the weights of data that was mislabelled.\n\nHowever, Adaboost is extremely sensitive to noisy data and outliers, which is why it is highly recommended to remove outliers from data with Adaboost.\n\nHere, Adaboost was used on our heart disease dataset, with a mean squared error of 0.12 on our testing data, and an overall accuracy score of 0.880 and a precision score of 0.880. There were 13 false positives and 9 false negatives. This is a decent score for a machine learning model, but when it comes to diagnosis it probably isn't the best and there is certainly a more ideal model. \n\n","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"27b6e5","input":"Support Vector Classification Model uses two groups of data to categorize new data.  It does this by plotting the data onto a graph, and then using a hyperplane to split the data according to the group that the data belongs to. The hyper\\-parameters  that could be fine tuned are the Kernel, the C value, the Gamma value, and the Degree. I decided to use a RandomizedSearchCV to determine the best parameters.  When testing, for the best parameters, I ran into some problems. The first problem was how long it took too long to run the program. The program would run for about thirty minutes, and still not find a best fit for the graph. In response, I decided to decrease the amount of variables the RandomizedSearchCV needed to look for, and additionally, the range that each variable could search within. In the end, I decided that testing the C value, the Gamma value, and the Kernel would be most helpful in optimizing the model. According to my values the RandomizedSearchCV determine the best parameters was a Kernel of linear, a Gamma value of 96, and a C value of 9.\n\n","pos":51,"type":"cell"}
{"cell_type":"markdown","id":"2b26f7","input":"grid search cv","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"6fa824","input":"","pos":29,"type":"cell"}
{"cell_type":"markdown","id":"87453c","input":"The model is a logistic regression and it works by predicting whether someone has heart disease or if they do not. It classifies them into one of these two groups . We changed some parameters such as the weight, training and testing data. I think it performed well in predicting if someone has heart disease and classifying them into two groups. It does this by analyzing relationships between two independent variables. The coefficient of the linear regression model shows the relationship between the variables by multiplying it with a certain value and a higher/lower coefficient indicates which classification the data point would be predicted to be in.\n\n","pos":24,"type":"cell"}
{"cell_type":"markdown","id":"b04b22","input":"The KNN nearest neighbors function looks for the k nearest neighbors of a point and their classification and then classifies the point based on the state of the nearest neighbors. Using GridSearchCV, I tried to optimize the value of the k nearest neighbors in a range from 1 to 400 and it was found that 23 was the most optimal number. It ended up giving a success rate of roughly 72.8 percent with 25 false positives and 25 false negative, 57 correct positives, and 77 correct negatives, a recall rate of 75.5 percent a precision rate of 75.5 percent, and an f1\\_score of 75.5 percent. These numbers would be described as a moderately \\-to good accurate model.\n\n","pos":19,"type":"cell"}
{"cell_type":"markdown","id":"c62dc8","input":"Gaussian Naive Bayes is for binary classification and assumes that all continuous data has normal distribution, and that all features are independent and equal from each other. The Naive Bayes finds the probability of the predicted outcome\\(y variable\\) given the x values and then classifies the point based on whether one of the classifications had a higher probability than the other classification. In order to optimize the function, I stratified the data, which raised the accuracy score from 0.83 to 0.885 while also scaling the data though that did not cause a change in the accuracy score or any of the other measures. The results ending up giving a accuracy score of 0.885, a precision score of 0.878, a recall score of 0.921, and an f1 score of 0.899. This resulted in 8 false negatives and 13 false positives out of 184 points in what I believe can be described as an accurate model.\n\n","pos":33,"type":"cell"}
{"id":0,"time":1659626150279,"type":"user"}
{"last_load":1659622169148,"type":"file"}